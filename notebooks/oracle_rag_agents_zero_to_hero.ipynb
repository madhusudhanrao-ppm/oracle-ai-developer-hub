{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e322ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -Uq oracledb sqlalchemy pandas sentence-transformers datasets einops \"numpy<2.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010cf528",
   "metadata": {},
   "source": [
    "# Part 1. Oracle AI Database Local Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20816044",
   "metadata": {},
   "source": [
    "## 1.1 Local Installation of Oracle AI Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c687fd63",
   "metadata": {},
   "source": [
    "**Oracle AI Database 26ai** is a **converged database** built for AI developers.  \n",
    "It unifies **relational, document, graph and vector data** in one engine ‚Äî so you can build  \n",
    "**semantic search**, **RAG**, and **natural language to SQL** applications without leaving the database.  \n",
    "\n",
    "Store embeddings, run vector search, and apply AI directly where your data lives ‚Äî  \n",
    "**securely, efficiently, and at scale.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ad10f4",
   "metadata": {},
   "source": [
    "For this notebook we will be using a local installation of [Oracle AI Database](https://www.oracle.com/database/free/get-started/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0308b61d",
   "metadata": {},
   "source": [
    "1. Install & start Docker. Docker Desktop (Mac/Windows) or Docker Engine (Linux). Make sure it‚Äôs running.\n",
    "    - If installed with Docker Enginer, run from terminal ```open /Applications/Docker.app```\n",
    "2. Pull [docker image](https://www.oracle.com/database/free/get-started/)\n",
    "3. Run a container with oracle image\n",
    "\n",
    "    ```\n",
    "    docker run -d \\\n",
    "      --name oracle-full \\\n",
    "      -p 1521:1521 -p 5500:5500 \\\n",
    "      -e ORACLE_PWD=OraclePwd_2025 \\\n",
    "      -e ORACLE_SID=FREE \\\n",
    "      -e ORACLE_PDB=FREEPDB1 \\\n",
    "      -v ~/oracle/full_data:/opt/oracle/oradata \\\n",
    "      container-registry.oracle.com/database/free:latest\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0da924",
   "metadata": {},
   "source": [
    "> üö´ **Troubleshoot**  \n",
    "> If you see the error:  \n",
    "> *`docker: Error response from daemon: Conflict. The container name \"/oracle-full\" is already in use by container ... You have to remove (or rename) that container to be able to reuse that name.`*  \n",
    ">\n",
    "> üß© **Fix:**  \n",
    "> - Remove the existing container:  \n",
    ">   ```bash\n",
    ">   docker rm oracle-full\n",
    ">   ```  \n",
    "> - Then re-run your Docker command from **Step 3** to start a new container.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c21e0d",
   "metadata": {},
   "source": [
    "After running the docker command above in your terminal, you should see the image below if you click into the container running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61486cb",
   "metadata": {},
   "source": [
    "![Docker Container Log](./images/docker_container_image.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464c1481",
   "metadata": {},
   "source": [
    "### 1.1.1 Connecting to Oracle AI Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de8defa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection attempt 1/3...\n",
      "‚úì Connected successfully!\n",
      "\n",
      "Oracle AI Database 26ai Free Release 23.26.0.0.0 - Develop, Learn, and Run for Free\n"
     ]
    }
   ],
   "source": [
    "import oracledb\n",
    "import time\n",
    "\n",
    "def connect_to_oracle(max_retries=3, retry_delay=5):\n",
    "    \"\"\"\n",
    "    Connect to Oracle database with retry logic and better error handling.\n",
    "    \n",
    "    Args:\n",
    "        max_retries: Maximum number of connection attempts\n",
    "        retry_delay: Seconds to wait between retries\n",
    "    \"\"\"\n",
    "    user = \"system\"\n",
    "    password = \"OraclePwd_2025\"  # must match ORACLE_PWD from docker run\n",
    "    dsn = \"localhost:1521/FREEPDB1\"\n",
    "    \n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            print(f\"Connection attempt {attempt}/{max_retries}...\")\n",
    "            conn = oracledb.connect(\n",
    "                user=user,\n",
    "                password=password,\n",
    "                dsn=dsn\n",
    "            )\n",
    "            print(\"‚úì Connected successfully!\")\n",
    "            \n",
    "            # Test the connection\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(\"SELECT banner FROM v$version WHERE banner LIKE 'Oracle%';\")\n",
    "                banner = cur.fetchone()[0]\n",
    "                print(f\"\\n{banner}\")\n",
    "            \n",
    "            return conn\n",
    "            \n",
    "        except oracledb.OperationalError as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"‚úó Connection failed (attempt {attempt}/{max_retries})\")\n",
    "            \n",
    "            if \"DPY-4011\" in error_msg or \"Connection reset by peer\" in error_msg:\n",
    "                print(\"  ‚Üí This usually means:\")\n",
    "                print(\"    1. Database is still starting up (wait 2-3 minutes)\")\n",
    "                print(\"    2. Listener is not bound to 0.0.0.0 (run fix_oracle_listener())\")\n",
    "                print(\"    3. Container is not running (check with check_docker_container())\")\n",
    "                \n",
    "                if attempt < max_retries:\n",
    "                    print(f\"\\n  Waiting {retry_delay} seconds before retry...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                else:\n",
    "                    print(\"\\n  üí° Try running:\")\n",
    "                    print(\"     1. check_docker_container() - verify container is running\")\n",
    "                    print(\"     2. fix_oracle_listener() - fix listener binding\")\n",
    "                    raise\n",
    "            else:\n",
    "                raise\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Unexpected error: {e}\")\n",
    "            raise\n",
    "    \n",
    "    raise ConnectionError(\"Failed to connect after all retries\")\n",
    "\n",
    "# Connect to Oracle\n",
    "conn = connect_to_oracle()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31988f1d",
   "metadata": {},
   "source": [
    "> üö´ Troubleshoot: Oracle Database Free (Docker) ‚Äî Connection Fix\n",
    ">\n",
    "> If you see errors like:\n",
    ">\n",
    "> ```\n",
    "> OperationalError: DPY-6005: cannot connect to database\n",
    "> DPY-4011: the database or network closed the connection\n",
    "> TNS-12545: Connect failed because target host or object does not exist\n",
    "> ```\n",
    ">\n",
    "> It means the **Oracle listener** inside the container is binding to the **container hostname** instead of `0.0.0.0`, preventing host connections.\n",
    ">\n",
    ">\n",
    ">\n",
    "> üß© Fix\n",
    "> \n",
    "> Run this exact command to patch the listener and restart it:\n",
    "> \n",
    "> ```bash\n",
    "> docker exec -it oracle-full bash -lc '\n",
    ">   export ORACLE_HOME=${ORACLE_HOME:-/opt/oracle/product/26ai/dbhomeFree}\n",
    ">   export PATH=$ORACLE_HOME/bin:$PATH\n",
    ">   LISTENER_ORA=\"$ORACLE_HOME/network/admin/listener.ora\"\n",
    "> \n",
    ">   echo \"== Fixing listener to use HOST=0.0.0.0\"\n",
    ">   sed -i \"s/(HOST *= *[^)]*)/(HOST = 0.0.0.0)/\" \"$LISTENER_ORA\"\n",
    "> \n",
    ">   echo \"== Restarting listener\"\n",
    ">   lsnrctl stop || true\n",
    ">   lsnrctl start\n",
    "> \n",
    ">   echo \"== Re-registering services\"\n",
    ">   echo \"ALTER SYSTEM REGISTER;\" | sqlplus -s / as sysdba\n",
    "> \n",
    ">   echo \"== Listener status (first 20 lines):\"\n",
    ">   lsnrctl status | sed -n \"1,20p\"\n",
    "> '\n",
    "> ```\n",
    "> \n",
    "> This:\n",
    "> - Forces the listener to bind on all interfaces (`0.0.0.0`).\n",
    "> - Restarts the listener.\n",
    "> - Re-registers the PDB service (`FREEPDB1`) with the listener.\n",
    "> \n",
    "> ---\n",
    "> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d714bb2",
   "metadata": {},
   "source": [
    "## 1.2 Remote Access with FreeSQL.com (Coming Soon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c24fa8",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02032d60",
   "metadata": {},
   "source": [
    "# Part 2. Data Loading, Preparation and Embedding Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d8e19e",
   "metadata": {},
   "source": [
    "## 2.1 Data Loading From Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6ddae6",
   "metadata": {},
   "source": [
    "**Streaming the ArXiv Papers Dataset with Hugging Face**\n",
    "\n",
    "The following code in the next cell demonstrates how to efficiently load and stream a large dataset using the **Hugging Face `datasets`** library ‚Äî a powerful tool for handling massive datasets that may not fit into memory all at once.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds_stream = load_dataset(\"nick007x/arxiv-papers\", split=\"train\", streaming=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6cc928",
   "metadata": {},
   "source": [
    "**Step-by-Step Explanation**\n",
    "1. **Importing dependencies**\n",
    "   - `load_dataset` is imported from the `datasets` library, giving access to thousands of open datasets hosted on the Hugging Face Hub.\n",
    "\n",
    "2. **Loading the dataset**\n",
    "   - The dataset `\"nick007x/arxiv-papers\"` refers to a public dataset on the Hugging Face Hub that contains metadata or text from research papers hosted on [arXiv.org](https://arxiv.org).\n",
    "   - The parameter `split=\"train\"` loads the training partition of the dataset (many datasets have `train`, `validation`, and `test` splits).\n",
    "   - The key argument `streaming=True` activates **streaming mode**, meaning the dataset is read progressively from the source without downloading it entirely to disk.\n",
    "\n",
    "3. **Why streaming mode matters**\n",
    "   - Traditional dataset loading downloads the full dataset into memory or disk, which can be slow and memory-intensive.  \n",
    "   - Streaming allows you to process examples **as they arrive**, ideal for very large datasets or limited-resource environments.\n",
    "   - You can iterate over the dataset like this:\n",
    "     ```python\n",
    "     for record in ds_stream:\n",
    "         print(record)\n",
    "         break\n",
    "     ```\n",
    "\n",
    "4. **Resulting object**\n",
    "   - The variable `ds_stream` is an instance of `datasets.IterableDataset`, not a static table.  \n",
    "   - You can convert a small sample into a Pandas DataFrame for inspection:\n",
    "     ```python\n",
    "     sample = [next(iter(ds_stream)) for _ in range(5)]\n",
    "     pd.DataFrame(sample)\n",
    "     ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b06e64",
   "metadata": {},
   "source": [
    "> **üí° Takeaway:**  \n",
    "> Using `load_dataset(..., streaming=True)` enables developers and data scientists to work with **large datasets efficiently** ‚Äî a perfect fit for machine learning pipelines, LLM training, or large document analysis workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43ee654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds_stream = load_dataset(\"nick007x/arxiv-papers\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8e1adc",
   "metadata": {},
   "source": [
    "The code below streams the first 1,000 ArXiv papers(feel free to use more) from the dataset, extracts their titles and abstracts, combines them into a single text field, and stores the results as structured dictionaries for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5ebcc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Streamed 1000 papers.\n"
     ]
    }
   ],
   "source": [
    "sampled = []\n",
    "for i, item in enumerate(ds_stream):\n",
    "    if i >= 1000:\n",
    "        break\n",
    "    \n",
    "    arxiv_id = item.get(\"arxiv_id\", f\"unknown_{i}\")\n",
    "    title = item.get(\"title\", \"\").strip()\n",
    "    abstract = item.get(\"abstract\", \"\").strip()\n",
    "    \n",
    "    # Combine title + abstract for embedding text\n",
    "    text = f\"{title} ‚Äî {abstract}\"\n",
    "    \n",
    "    sampled.append({\n",
    "        \"id\": item.get(\"id\", f\"arxiv_{i}\"),\n",
    "        \"arxiv_id\": arxiv_id,\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"text\": text\n",
    "    })\n",
    "\n",
    "print(f\"‚úÖ Streamed {len(sampled)} papers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaae87a",
   "metadata": {},
   "source": [
    "The code below converts the collected list of sampled paper records into a Pandas DataFrame for easier analysis, prints how many rows were loaded, and displays the first few entries to preview the dataset‚Äôs structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b655b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 1000 rows into DataFrame.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arxiv_0</td>\n",
       "      <td>0902.3253</td>\n",
       "      <td>The gravitational wave background from star-ma...</td>\n",
       "      <td>Stars on eccentric orbits around a massive bla...</td>\n",
       "      <td>The gravitational wave background from star-ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arxiv_1</td>\n",
       "      <td>0902.0428</td>\n",
       "      <td>Dynamics of planets in retrograde mean motion ...</td>\n",
       "      <td>In a previous paper (Gayon &amp;amp; Bois 2008a), ...</td>\n",
       "      <td>Dynamics of planets in retrograde mean motion ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arxiv_2</td>\n",
       "      <td>0901.3401</td>\n",
       "      <td>Diurnal Thermal Tides in a Non-synchronized Ho...</td>\n",
       "      <td>We perform a linear analysis to investigate th...</td>\n",
       "      <td>Diurnal Thermal Tides in a Non-synchronized Ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arxiv_3</td>\n",
       "      <td>0901.1570</td>\n",
       "      <td>Intermittent turbulence, noisy fluctuations an...</td>\n",
       "      <td>Recent research has shown that distinct physic...</td>\n",
       "      <td>Intermittent turbulence, noisy fluctuations an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arxiv_4</td>\n",
       "      <td>0901.2048</td>\n",
       "      <td>Falling Transiting Extrasolar Giant Planets</td>\n",
       "      <td>We revisit the tidal stability of extrasolar s...</td>\n",
       "      <td>Falling Transiting Extrasolar Giant Planets ‚Äî ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   arxiv_id                                              title  \\\n",
       "0  arxiv_0  0902.3253  The gravitational wave background from star-ma...   \n",
       "1  arxiv_1  0902.0428  Dynamics of planets in retrograde mean motion ...   \n",
       "2  arxiv_2  0901.3401  Diurnal Thermal Tides in a Non-synchronized Ho...   \n",
       "3  arxiv_3  0901.1570  Intermittent turbulence, noisy fluctuations an...   \n",
       "4  arxiv_4  0901.2048        Falling Transiting Extrasolar Giant Planets   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Stars on eccentric orbits around a massive bla...   \n",
       "1  In a previous paper (Gayon &amp; Bois 2008a), ...   \n",
       "2  We perform a linear analysis to investigate th...   \n",
       "3  Recent research has shown that distinct physic...   \n",
       "4  We revisit the tidal stability of extrasolar s...   \n",
       "\n",
       "                                                text  \n",
       "0  The gravitational wave background from star-ma...  \n",
       "1  Dynamics of planets in retrograde mean motion ...  \n",
       "2  Diurnal Thermal Tides in a Non-synchronized Ho...  \n",
       "3  Intermittent turbulence, noisy fluctuations an...  \n",
       "4  Falling Transiting Extrasolar Giant Planets ‚Äî ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the list of tuples (id, text) into a DataFrame\n",
    "dataset_df = pd.DataFrame(sampled)\n",
    "\n",
    "# View shape and head\n",
    "print(f\"‚úÖ Loaded {len(dataset_df)} rows into DataFrame.\")\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ac879d",
   "metadata": {},
   "source": [
    "## 2.2 Embedding Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357d2c62",
   "metadata": {},
   "source": [
    "\n",
    "This code in the next cell below imports the **`SentenceTransformer`** class from the `sentence_transformers` library and loads a pretrained model called **`\"nomic-ai/nomic-embed-text-v1.5\"`** from the Hugging Face Hub.\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)\n",
    "```\n",
    "\n",
    "üîç What it does\n",
    "- **`SentenceTransformer`** extends transformer-based models (like BERT or RoBERTa) to produce **sentence-level embeddings** ‚Äî dense numerical vectors that capture the semantic meaning of text.\n",
    "- The model **`nomic-ai/nomic-embed-text-v1.5`** is optimized for general-purpose text embeddings and works well for tasks such as:\n",
    "  - Semantic search  \n",
    "  - Clustering and topic modeling  \n",
    "  - Retrieval-Augmented Generation (RAG)  \n",
    "  - Similarity ranking and recommendation systems\n",
    "- The parameter **`trust_remote_code=True`** allows the loader to execute custom code from the model‚Äôs repository, which is required for models that define specialized architectures or preprocessing logic.\n",
    "\n",
    "In short, this code prepares a powerful embedding model that can transform text (like paper titles or abstracts) into **high-dimensional semantic vectors**, making it easier to measure meaning-based similarity across documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "939be3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeb4f07",
   "metadata": {},
   "source": [
    "This code in the next cell below iterates through each document, encodes it into a normalized embedding vector while showing live progress, and prefixes each text with \"search_document: \" so that the Nomic embedding model correctly interprets it as a retrieval document, improving alignment with query embeddings during semantic search.\n",
    "\n",
    "The prefix `search_document`: \" tells the Nomic embedding model what kind of text it‚Äôs encoding ‚Äî in this case, a document intended for retrieval.\n",
    "\n",
    "Nomic models like nomic-embed-text-v1.5 are trained with instructional prefixes (e.g., \"search_query:\", \"search_document:\", \"classification:\"), which guide the model to generate embeddings suited for different purposes.\n",
    "\n",
    "**Why Use the `\"search_document:\"` Prefix with Nomic Embeddings**\n",
    "\n",
    "Nomic embedding models (like **`nomic-embed-text-v1.5`**) are **instruction-tuned**, meaning they were trained with specific **task prefixes** that tell the model *how* to interpret the text you‚Äôre embedding.  \n",
    "\n",
    "According to the [Nomic documentation](https://docs.nomic.ai/reference/api/embed-text-v-1-embedding-text-post) and the [Hugging Face model card](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5):\n",
    "\n",
    "> ‚ÄúImportant: the text prompt must include a task instruction prefix, instructing the model which task is being performed.\n",
    "> For example, if you are implementing a RAG application, you embed your documents as  \n",
    "> `search_document: <text>` and embed your user queries as `search_query: <text>`.‚Äù\n",
    "\n",
    "**üí° Why this matters**\n",
    "- The prefix tells the model whether a text is a **document** or a **query**, ensuring both are embedded into the **same semantic space**.\n",
    "- Using `search_document:` for document embeddings and `search_query:` for query embeddings **improves retrieval accuracy**, since the model optimizes for similarity between matching query‚Äìdocument pairs.\n",
    "- Omitting or mismatching prefixes can lead to weaker alignment and lower recall in search or RAG workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be75832a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 1000/1000 texts...\n",
      "‚úÖ Finished encoding 1000 texts.\n",
      "‚úÖ Embeddings shape: (1000, 768)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Prefix for retrieval-style embeddings\n",
    "dataset_df[\"text_prefixed\"] = dataset_df[\"text\"].apply(lambda x: f\"search_document: {x}\")\n",
    "\n",
    "# Convert to list for iteration\n",
    "texts = dataset_df[\"text_prefixed\"].tolist()\n",
    "embs = []\n",
    "\n",
    "# Encode one text at a time with a single-line progress display\n",
    "total = len(texts)\n",
    "for i, text in enumerate(texts, start=1):\n",
    "    embedding = embedding_model.encode(\n",
    "        text,\n",
    "        show_progress_bar=False,   # Disable SentenceTransformer progress\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "    embs.append(embedding)\n",
    "\n",
    "    # Print progress on the same line\n",
    "    sys.stdout.write(f\"\\rEncoding {i}/{total} texts...\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# Final newline to avoid overwriting the last line\n",
    "print(f\"\\n‚úÖ Finished encoding {len(embs)} texts.\")\n",
    "\n",
    "# Convert list of vectors to NumPy array\n",
    "embs = np.vstack(embs)\n",
    "print(f\"‚úÖ Embeddings shape: {embs.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2986b5f5",
   "metadata": {},
   "source": [
    "One important detail to note is the embedding dimensionality ‚Äî the number of numerical features in each vector representation.\n",
    "This dimensionality determines the structure of your vector index and must remain consistent across all embeddings to ensure efficient similarity search and accurate retrieval performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab5614e",
   "metadata": {},
   "source": [
    "This code in the next cell below converts each embedding into a list of 32-bit floating-point numbers (float32) so it can be stored in an Oracle VECTOR column, determines the embedding dimension (needed for defining the vector index), and we then displays the first two rows to confirm the data and embeddings were formatted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53954b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>text</th>\n",
       "      <th>text_prefixed</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arxiv_0</td>\n",
       "      <td>0902.3253</td>\n",
       "      <td>The gravitational wave background from star-ma...</td>\n",
       "      <td>Stars on eccentric orbits around a massive bla...</td>\n",
       "      <td>The gravitational wave background from star-ma...</td>\n",
       "      <td>search_document: The gravitational wave backgr...</td>\n",
       "      <td>[0.03310789167881012, 0.06245185434818268, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arxiv_1</td>\n",
       "      <td>0902.0428</td>\n",
       "      <td>Dynamics of planets in retrograde mean motion ...</td>\n",
       "      <td>In a previous paper (Gayon &amp;amp; Bois 2008a), ...</td>\n",
       "      <td>Dynamics of planets in retrograde mean motion ...</td>\n",
       "      <td>search_document: Dynamics of planets in retrog...</td>\n",
       "      <td>[0.029524987563490868, 0.07589567452669144, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   arxiv_id                                              title  \\\n",
       "0  arxiv_0  0902.3253  The gravitational wave background from star-ma...   \n",
       "1  arxiv_1  0902.0428  Dynamics of planets in retrograde mean motion ...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Stars on eccentric orbits around a massive bla...   \n",
       "1  In a previous paper (Gayon &amp; Bois 2008a), ...   \n",
       "\n",
       "                                                text  \\\n",
       "0  The gravitational wave background from star-ma...   \n",
       "1  Dynamics of planets in retrograde mean motion ...   \n",
       "\n",
       "                                       text_prefixed  \\\n",
       "0  search_document: The gravitational wave backgr...   \n",
       "1  search_document: Dynamics of planets in retrog...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.03310789167881012, 0.06245185434818268, -0....  \n",
       "1  [0.029524987563490868, 0.07589567452669144, -0...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store as float32 lists (ready for Oracle VECTOR column)\n",
    "dataset_df[\"embedding\"] = [e.astype(np.float32).tolist() for e in embs]\n",
    "\n",
    "dim = len(dataset_df[\"embedding\"].iloc[0])\n",
    "\n",
    "# View the first 2 rows of the dataset\n",
    "dataset_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58889766",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c526f50c",
   "metadata": {},
   "source": [
    "# Part 3: Database Table Setup and Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9710e3",
   "metadata": {},
   "source": [
    "This code below drops and recreates a table called research_papers with columns for paper metadata and a VECTOR column that stores embeddings of size `dim` (the embedding dimension you computed earlier), enabling Oracle to perform vector search on those embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a52cfc9",
   "metadata": {},
   "source": [
    "## 3.1 Create Reseach Papers Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "833b3c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl = f\"\"\"\n",
    "BEGIN\n",
    "    EXECUTE IMMEDIATE 'DROP TABLE research_papers';\n",
    "EXCEPTION WHEN OTHERS THEN\n",
    "    IF SQLCODE != -942 THEN RAISE; END IF;\n",
    "END;\n",
    "/\n",
    "CREATE TABLE research_papers (\n",
    "    arxiv_id VARCHAR2(255) PRIMARY KEY,\n",
    "    title VARCHAR2(4000),\n",
    "    abstract VARCHAR2(4000),\n",
    "    text CLOB,\n",
    "    embedding VECTOR({dim}, FLOAT32)\n",
    ")\n",
    "TABLESPACE USERS\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f83f9e",
   "metadata": {},
   "source": [
    "Here, we‚Äôre taking the multi-statement SQL stored in `ddl`, splitting it by `/` so each command runs separately, and executing them one by one using the database cursor.  \n",
    "After all statements finish, we call `conn.commit()` to save the changes ‚Äî effectively creating the `RESEARCH_PAPERS` table ‚Äî and then print a confirmation message showing the vector dimension (`dim`) used for the `embedding` column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d20eecfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Table RESEARCH_PAPERS created with VECTOR dimension: 768\n"
     ]
    }
   ],
   "source": [
    "with conn.cursor() as cur:\n",
    "    for stmt in ddl.split(\"/\"):\n",
    "        if stmt.strip():\n",
    "            cur.execute(stmt)\n",
    "\n",
    "conn.commit()\n",
    "print(\"‚úÖ Table RESEARCH_PAPERS created with VECTOR dimension:\", dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2ab118",
   "metadata": {},
   "source": [
    "## 3.2 Create Indexes (Vector and Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0e7ce7",
   "metadata": {},
   "source": [
    "This code in the next cell below creates a **vector index** on the `embedding` column of the `research_papers` table to enable fast similarity search.  \n",
    "The index, named `RP_VEC_IVF`, uses Oracle‚Äôs `VECTOR` indexing with **IVF (Inverted File)** organization, which partitions vectors into clusters for efficient nearest-neighbor lookup.  \n",
    "It‚Äôs configured to use **cosine distance** as the similarity metric and aims for a **target accuracy of 90%**, balancing search speed and precision.  \n",
    "Finally, `conn.commit()` saves the index creation, and a confirmation message is printed once the index is successfully built.\n",
    "\n",
    "Note: In Oracle‚Äôs vector indexing syntax, **ORGANIZATION NEIGHBOR PARTITIONS** specifies that the index should be built using the IVF (Inverted File) structure ‚Äî a partition-based nearest-neighbor organization optimized for large-scale vector search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87b97672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector Index RP_VEC_IVF created\n"
     ]
    }
   ],
   "source": [
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE VECTOR INDEX RP_VEC_IVF\n",
    "        ON research_papers(embedding)\n",
    "        ORGANIZATION NEIGHBOR PARTITIONS\n",
    "        DISTANCE COSINE\n",
    "        WITH TARGET ACCURACY 90\n",
    "        TABLESPACE USERS\n",
    "    \"\"\" )\n",
    "\n",
    "conn.commit()\n",
    "print(\"‚úÖ Vector Index RP_VEC_IVF created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106fa5b5",
   "metadata": {},
   "source": [
    "> **üí° Knowledge Checkpoint**\n",
    ">\n",
    "> IVF (Inverted File Index) is a vector indexing technique that speeds up similarity search by clustering vectors into groups (called partitions or  centroids).\n",
    ">\n",
    "> Instead of comparing a query vector to every vector in the dataset, IVF first identifies the most relevant clusters and only searches within those > ‚Äî drastically reducing computation.\n",
    "> \n",
    "> In Oracle AI Database, the clause ORGANIZATION NEIGHBOR PARTITIONS activates this IVF-style structure, allowing the database to perform approximate nearest-neighbor (ANN) searches efficiently while maintaining high accuracy (controlled by the WITH TARGET ACCURACY parameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6231597e",
   "metadata": {},
   "source": [
    "**Creating an Oracle Text Index for Full-Text Search**\n",
    "\n",
    "This code below creates a **full-text search index** on the `text` column of the `research_papers` table using Oracle Text.\n",
    "\n",
    "1. **Drop existing index** ‚Äî removes `rp_text_idx` if it already exists, ignoring the ‚Äúindex does not exist‚Äù error.  \n",
    "2. **Create new text index** ‚Äî builds a `CTXSYS.CONTEXT` index, which tokenizes and indexes the text for efficient keyword searches.  \n",
    "   - `SYNC (ON COMMIT)` keeps the index automatically updated whenever new data is committed.  \n",
    "3. **Commit and confirm** ‚Äî saves the changes and prints a success message.\n",
    "\n",
    "Once created, you can use the `CONTAINS()` operator with `SCORE()` for fast, ranked keyword searches, e.g.:\n",
    "\n",
    "```sql\n",
    "SELECT title, SCORE(1) AS relevance\n",
    "FROM research_papers\n",
    "WHERE CONTAINS(text, 'transformer architecture', 1) > 0\n",
    "ORDER BY SCORE(1) DESC;\n",
    "```\n",
    "\n",
    "This turns your text column into a search-optimized field, similar to how search engines handle full-text retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52720503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Oracle Text index (rp_text_idx) created successfully on TEXT column.\n"
     ]
    }
   ],
   "source": [
    "with conn.cursor() as cur:\n",
    "    # Drop old index if it exists\n",
    "    try:\n",
    "        cur.execute(\"DROP INDEX rp_text_idx\")\n",
    "    except oracledb.DatabaseError as e:\n",
    "        if \"ORA-01418\" not in str(e):  # ignore \"index does not exist\"\n",
    "            raise\n",
    "\n",
    "    # Create a TEXT index on the 'text' column only\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE INDEX rp_text_idx\n",
    "        ON research_papers(text)\n",
    "        INDEXTYPE IS CTXSYS.CONTEXT\n",
    "        PARAMETERS ('SYNC (ON COMMIT)')\n",
    "    \"\"\")\n",
    "\n",
    "conn.commit()\n",
    "print(\"‚úÖ Oracle Text index (rp_text_idx) created successfully on TEXT column.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2d74db",
   "metadata": {},
   "source": [
    "In the code below we‚Äôre taking all the research paper records (including their embeddings) from the DataFrame and inserting them into the `RESEARCH_PAPERS` table in Oracle.\n",
    "\n",
    "Let‚Äôs break down what‚Äôs happening:\n",
    "\n",
    "\n",
    "1. Convert each embedding for Oracle compatibility\n",
    "```python\n",
    "embedding_array = array.array('f', row.get(\"embedding\"))\n",
    "```\n",
    "Oracle‚Äôs `VECTOR` column expects the data as a compact binary float array, not a Python list.  \n",
    "So here we convert each embedding into an `array.array('f')` ‚Äî this ensures the data binds correctly and efficiently when inserting.\n",
    "\n",
    "\n",
    "2. Prepare all rows for insertion\n",
    "For every paper, we build a tuple containing its metadata (`arxiv_id`, `title`, `abstract`, `text`) and the formatted `embedding_array`.  \n",
    "These tuples are collected in a list called `rows`.\n",
    "\n",
    "3. Insert each row with a progress bar\n",
    "```python\n",
    "for row in tqdm(rows):\n",
    "    cur.execute(\"\"\"\n",
    "        INSERT INTO research_papers (arxiv_id, title, abstract, text, embedding)\n",
    "        VALUES (:1, :2, :3, :4, :5)\n",
    "    \"\"\", row)\n",
    "```\n",
    "We loop through each row, inserting it into the table using Oracle‚Äôs parameterized query syntax.  \n",
    "The `tqdm` progress bar gives real-time feedback on the insertion process ‚Äî helpful when inserting hundreds or thousands of embeddings.\n",
    "\n",
    "\n",
    "4. Commit everything\n",
    "Finally, `conn.commit()` saves all the inserted records permanently in the database.  You‚Äôll see a confirmation message once the operation completes successfully.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08324270",
   "metadata": {},
   "source": [
    "## 3.3 Ingest Data into Oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be6bdde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to insert 1000 rows into RESEARCH_PAPERS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:01<00:00, 846.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data inserted successfully\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import array\n",
    "\n",
    "\n",
    "rows = []\n",
    "for i, row in dataset_df.iterrows():\n",
    "    # Convert embedding list to array.array for proper VECTOR binding\n",
    "    embedding_array = array.array('f', row.get(\"embedding\"))\n",
    "    \n",
    "    rows.append((\n",
    "        row.get(\"arxiv_id\"),\n",
    "        row.get(\"title\"),\n",
    "        row.get(\"abstract\"),\n",
    "        row.get(\"text\"),\n",
    "        embedding_array\n",
    "    ))\n",
    "\n",
    "print(f\"Preparing to insert {len(rows)} rows into RESEARCH_PAPERS...\")\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    for row in tqdm(rows):\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO research_papers (arxiv_id, title, abstract, text, embedding)\n",
    "            VALUES (:1, :2, :3, :4, :5)\n",
    "            \"\"\", \n",
    "            row\n",
    "        )\n",
    "        \n",
    "conn.commit()\n",
    "print(\"‚úÖ Data inserted successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7a14d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count: 1000\n",
      "('0902.3253', 'The gravitational wave background from star-massive black hole fly-bys', 'Stars on eccentric orbits around a massive black hole (MBH) emit bursts of gravitational waves (GWs) at periapse. Such events may be directly resolvable in the Galactic centre. However, if the star does not spiral in, the emitted GWs are not resolvable for extra-galactic MBHs, but constitute a source of background noise. We estimate the power spectrum of this extreme mass ratio burst background (EMBB) and compare it to the anticipated instrumental noise of the Laser Interferometer Space Antenna (LISA). To this end, we model the regions close to a MBH, accounting for mass-segregation, and for processes that limit the presence of stars close to the MBH, such as GW inspiral and hydrodynamical collisions between stars. We find that the EMBB is dominated by GW bursts from stellar mass black holes, and the magnitude of the noise spectrum (f S_GW)^{1/2} is at least a factor ~10 smaller than the instrumental noise. As an additional result of our analysis, we show that LISA is unlikely to detect relativistic bursts in the Galactic centre.', <oracledb.lob.LOB object at 0x15d68c610>)\n",
      "('0901.1570', 'Intermittent turbulence, noisy fluctuations and wavy structures in the Venusian magnetosheath and wake', 'Recent research has shown that distinct physical regions in the Venusian induced magnetosphere are recognizable from the variations of strength of the magnetic field and its wave/fluctuation activity. In this paper the statistical properties of magnetic fluctuations are investigated in the Venusian magnetosheath and wake regions. The main goal is to identify the characteristic scaling features of fluctuations along Venus Express (VEX) trajectory and to understand the specific circumstances of the occurrence of different types of scalings. For the latter task we also use the results of measurements from the previous missions to Venus. Our main result is that the changing character of physical interactions between the solar wind and the planetary obstacle is leading to different types of spectral scaling in the near-Venusian space. Noisy fluctuations are observed in the magnetosheath, wavy structures near the terminator and in the nightside near-planet wake. Multi-scale turbulence is observed at the magnetosheath boundary layer and near the quasi-parallel bow shock. Magnetosheath boundary layer turbulence is associated with an average magnetic field which is nearly aligned with the Sun-Venus line. Noisy magnetic fluctuations are well described with the Gaussian statistics. Both magnetosheath boundary layer and near shock turbulence statistics exhibit non-Gaussian features and intermittency over small spatio-temporal scales. The occurrence of turbulence near magnetosheath boundaries can be responsible for the local heating of plasma observed by previous missions.', <oracledb.lob.LOB object at 0x15d68ec90>)\n",
      "('0902.0428', 'Dynamics of planets in retrograde mean motion resonance', 'In a previous paper (Gayon &amp; Bois 2008a), we have shown the general efficiency of retrograde resonances for stabilizing compact planetary systems. Such retrograde resonances can be found when two-planets of a three-body planetary system are both in mean motion resonance and revolve in opposite directions. For a particular two-planet system, we have also obtained a new orbital fit involving such a counter-revolving configuration and consistent with the observational data. <br>In the present paper, we analytically investigate the three-body problem in this particular case of retrograde resonances. We therefore define a new set of canonical variables allowing to express correctly the resonance angles and obtain the Hamiltonian of a system harboring planets revolving in opposite directions. The acquiring of an analytical &#34;rail&#34; may notably contribute to a deeper understanding of our numerical investigation and provides the major structures related to the stability properties. A comparison between our analytical and numerical results is also carried out.', <oracledb.lob.LOB object at 0x15d6d7410>)\n"
     ]
    }
   ],
   "source": [
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"SELECT COUNT(*) FROM RESEARCH_PAPERS\")\n",
    "    print(\"Row count:\", cur.fetchone()[0])\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT arxiv_id, title, abstract, text FROM RESEARCH_PAPERS\n",
    "        FETCH FIRST 3 ROWS ONLY\n",
    "    \"\"\")\n",
    "    for row in cur.fetchall():\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9788fc",
   "metadata": {},
   "source": [
    "# Part 4. Retrieval Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab38912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_TEXT_KEYWORDS = \"optimization\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa03c53c",
   "metadata": {},
   "source": [
    "## 4.1 Text Based Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154d31cd",
   "metadata": {},
   "source": [
    "The code below performs a full-text search over the text column of the research_papers table, using the Oracle Text index we just created earlier\n",
    "\n",
    "- CONTAINS(text, :keyword, 1) uses the Oracle Text index on the text column to find documents containing the given keyword or phrase.\n",
    "- SCORE(1) assigns a relevance score based on how well each document matches the search term.\n",
    "- SUBSTR(text, 1, 200) returns the first 200 characters as a short preview snippet.\n",
    "- FETCH FIRST 10 ROWS ONLY limits the results to the top 10 most relevant matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7fbb1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_search_research_papers(conn, keyword: str):\n",
    "    \"\"\"\n",
    "    Perform a full-text keyword search on the 'text' column \n",
    "    using the Oracle Text index (rp_text_idx).\n",
    "\n",
    "    Args:\n",
    "        conn: Oracle database connection object.\n",
    "        keyword (str): Keyword or phrase to search for.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (rows, columns)\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            arxiv_id, \n",
    "            title, \n",
    "            SUBSTR(text, 1, 200) AS text_snippet,\n",
    "            SCORE(1) AS relevance_score\n",
    "        FROM research_papers\n",
    "        WHERE CONTAINS(text, :keyword, 1) > 0\n",
    "        ORDER BY SCORE(1) DESC\n",
    "        FETCH FIRST 10 ROWS ONLY\n",
    "    \"\"\"\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(query, keyword=keyword)\n",
    "        rows = cur.fetchall()\n",
    "        columns = [desc[0] for desc in cur.description]\n",
    "\n",
    "    return rows, columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbbe04fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Keyword Search: 'optimization'\n",
      "üìä Results: 3\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARXIV_ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>TEXT_SNIPPET</th>\n",
       "      <th>RELEVANCE_SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0912.1394</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0901.0846</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0904.4106</td>\n",
       "      <td>Is the HR 8799 extrasolar system destined for ...</td>\n",
       "      <td>Is the HR 8799 extrasolar system destined for ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ARXIV_ID                                              TITLE  \\\n",
       "0  0912.1394  Are Near Earth Objects the Key to Optimization...   \n",
       "1  0901.0846  A Metric and Optimisation Scheme for Microlens...   \n",
       "2  0904.4106  Is the HR 8799 extrasolar system destined for ...   \n",
       "\n",
       "                                        TEXT_SNIPPET  RELEVANCE_SCORE  \n",
       "0  Are Near Earth Objects the Key to Optimization...               42  \n",
       "1  A Metric and Optimisation Scheme for Microlens...               11  \n",
       "2  Is the HR 8799 extrasolar system destined for ...               11  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows, columns = keyword_search_research_papers(conn, SEARCH_TEXT_KEYWORDS)\n",
    "\n",
    "results_df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "print(f\"üîç Keyword Search: '{SEARCH_TEXT_KEYWORDS}'\")\n",
    "print(f\"üìä Results: {len(results_df)}\\n\")\n",
    "\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c352afa",
   "metadata": {},
   "source": [
    "As shown above, the returned rows represent documents whose indexed `text` content matched the full-text search query **\"Optimization\"**, as determined by the Oracle Text `CONTAINS()` operator using the `rp_text_idx` index.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ff4f6b",
   "metadata": {},
   "source": [
    "## 4.2 Vector Based Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d0cc6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_QUERY = \"Get me papers related to planetary exploration\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023c96d4",
   "metadata": {},
   "source": [
    "**Vector Similarity Search in Oracle**\n",
    "\n",
    "This function below `vector_search_research_papers` performs a **semantic vector search** on the `research_papers` table, retrieving papers most similar to a given query using **Oracle‚Äôs native VECTOR search** feature.\n",
    "\n",
    "\n",
    "Step-by-step overview\n",
    "\n",
    "1. Encode the search query\n",
    "```python\n",
    "query_embedding = embedding_model.encode(\n",
    "    [f\"search_query: {search_query}\"],\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")[0].astype(np.float32).tolist()\n",
    "```\n",
    "- The query is embedded using the same model that generated the document embeddings.  \n",
    "- The prefix `\"search_query:\"` ensures embeddings align with `\"search_document:\"` vectors.  \n",
    "- Normalized and cast to `float32` for Oracle‚Äôs `VECTOR` type.\n",
    "\n",
    "\n",
    "2. Prepare for database binding\n",
    "```python\n",
    "query_embedding_array = array.array('f', query_embedding)\n",
    "```\n",
    "- Converts the embedding list into a binary float array (`array('f')`), required for Oracle‚Äôs vector binding.\n",
    "\n",
    "\n",
    "3. Perform vector search\n",
    "```sql\n",
    "SELECT arxiv_id, title, abstract,\n",
    "       SUBSTR(text, 1, 200) AS text_snippet,\n",
    "        ROUND(1 - VECTOR_DISTANCE(embedding, :q, COSINE), 4) AS similarity_score\n",
    "FROM research_papers\n",
    "ORDER BY similarity_score\n",
    "FETCH APPROX FIRST {top_k} ROWS ONLY WITH TARGET ACCURACY 90\n",
    "```\n",
    "- Computes **cosine distance** between the query vector and stored embeddings.  \n",
    "- Orders results by similarity, returning the closest matches.  \n",
    "- Uses **Approximate Nearest Neighbor (ANN)** search for fast retrieval at 90% target accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b46a3f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search_research_papers(conn, embedding_model, search_query: str, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Perform a vector similarity search on the research_papers table using a query embedding.\n",
    "    Returns cosine similarity scores (higher = more similar).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1Ô∏è‚É£ Encode the query into a vector\n",
    "    query_embedding = embedding_model.encode(\n",
    "        [f\"search_query: {search_query}\"], \n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    )[0].astype(np.float32).tolist()\n",
    "\n",
    "    # 2Ô∏è‚É£ Prepare the vector for Oracle binding\n",
    "    query_embedding_array = array.array('f', query_embedding)\n",
    "\n",
    "    # 3Ô∏è‚É£ Run a vector similarity search using cosine similarity\n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            arxiv_id, \n",
    "            title, \n",
    "            abstract, \n",
    "            SUBSTR(text, 1, 200) AS text_snippet,\n",
    "            ROUND(1 - VECTOR_DISTANCE(embedding, :q, COSINE), 4) AS similarity_score\n",
    "        FROM research_papers\n",
    "        ORDER BY similarity_score DESC\n",
    "        FETCH APPROX FIRST {top_k} ROWS ONLY WITH TARGET ACCURACY 90\n",
    "    \"\"\"\n",
    "\n",
    "    # 4Ô∏è‚É£ Execute and return results\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(query, q=query_embedding_array)\n",
    "        rows = cur.fetchall()\n",
    "        columns = [desc[0] for desc in cur.description]\n",
    "\n",
    "    return rows, columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dca16bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Vector Search: 'optimization'\n",
      "üìä Results: 5\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARXIV_ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>TEXT_SNIPPET</th>\n",
       "      <th>SIMILARITY_SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0912.1394</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>This note suggests that near earth objects and...</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>0.7392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0901.0846</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>OGLE III and MOA II are discovering 600-1000 G...</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>0.6409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0911.2703</td>\n",
       "      <td>An Efficient Method for Modeling High Magnific...</td>\n",
       "      <td>I present a previously unpublished method for ...</td>\n",
       "      <td>An Efficient Method for Modeling High Magnific...</td>\n",
       "      <td>0.6136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001.0437</td>\n",
       "      <td>Kepler Science Operations</td>\n",
       "      <td>Kepler&amp;#39;s primary mission is a search for e...</td>\n",
       "      <td>Kepler Science Operations ‚Äî Kepler&amp;#39;s prima...</td>\n",
       "      <td>0.6019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0903.5139</td>\n",
       "      <td>Science-Operational Metrics and Issues for the...</td>\n",
       "      <td>A movement is underway to test the uniqueness ...</td>\n",
       "      <td>Science-Operational Metrics and Issues for the...</td>\n",
       "      <td>0.6009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ARXIV_ID                                              TITLE  \\\n",
       "0  0912.1394  Are Near Earth Objects the Key to Optimization...   \n",
       "1  0901.0846  A Metric and Optimisation Scheme for Microlens...   \n",
       "2  0911.2703  An Efficient Method for Modeling High Magnific...   \n",
       "3  1001.0437                          Kepler Science Operations   \n",
       "4  0903.5139  Science-Operational Metrics and Issues for the...   \n",
       "\n",
       "                                            ABSTRACT  \\\n",
       "0  This note suggests that near earth objects and...   \n",
       "1  OGLE III and MOA II are discovering 600-1000 G...   \n",
       "2  I present a previously unpublished method for ...   \n",
       "3  Kepler&#39;s primary mission is a search for e...   \n",
       "4  A movement is underway to test the uniqueness ...   \n",
       "\n",
       "                                        TEXT_SNIPPET  SIMILARITY_SCORE  \n",
       "0  Are Near Earth Objects the Key to Optimization...            0.7392  \n",
       "1  A Metric and Optimisation Scheme for Microlens...            0.6409  \n",
       "2  An Efficient Method for Modeling High Magnific...            0.6136  \n",
       "3  Kepler Science Operations ‚Äî Kepler&#39;s prima...            0.6019  \n",
       "4  Science-Operational Metrics and Issues for the...            0.6009  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows, columns = vector_search_research_papers(conn, embedding_model, SEARCH_TEXT_KEYWORDS, top_k=5)\n",
    "\n",
    "results_df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "print(f\"üîç Vector Search: '{SEARCH_TEXT_KEYWORDS}'\")\n",
    "print(f\"üìä Results: {len(results_df)}\\n\")\n",
    "\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c8bf3e",
   "metadata": {},
   "source": [
    "In this step, we used Oracle Database‚Äôs native vector search capabilities to perform a semantic similarity search over the research_papers dataset.\n",
    "\n",
    "The text query ‚Äî ‚ÄúGet me papers related to planetary exploration‚Äù ‚Äî was first transformed into a high-dimensional embedding vector using the same model that generated our document embeddings (nomic-embed-text-v1.5).\n",
    "This query vector captures the semantic meaning of the phrase rather than just the exact words.\n",
    "\n",
    "Using Oracle‚Äôs VECTOR_DISTANCE(..., COSINE) function (converted to 1 - distance), we then compared this query vector against the stored embeddings for each paper in the database.\n",
    "The result is a ranked list of research papers ordered by cosine similarity, where higher scores indicate stronger semantic alignment with the search query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93fd0c7",
   "metadata": {},
   "source": [
    "## 4.3 Hybrid Retrieval (Vector + Text Combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1972f31",
   "metadata": {},
   "source": [
    "#### 4.3.1 Pre Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e07236a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import array\n",
    "import numpy as np\n",
    "\n",
    "def hybrid_search_research_papers_pre_filter(\n",
    "    conn,\n",
    "    embedding_model,\n",
    "    search_phrase: str,\n",
    "    top_k: int = 10,\n",
    "    show_explain: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform a hybrid search using Oracle Text + Vector Search.\n",
    "    Combines lexical filtering (CONTAINS) with semantic re-ranking via cosine similarity.\n",
    "\n",
    "    Args:\n",
    "        conn: Oracle database connection object.\n",
    "        embedding_model: Model with `.encode()` method (e.g., SentenceTransformer).\n",
    "        search_phrase (str): User search phrase used for both text filtering and embedding.\n",
    "        top_k (int): Number of results to return (default = 10).\n",
    "        show_explain (bool): If True, prints the execution plan.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (rows, columns, exec_plan_text or None)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Step 1: Encode search phrase into normalized vector ---\n",
    "    query_embedding = embedding_model.encode(\n",
    "        [f\"search_query: {search_phrase}\"],\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    )[0].astype(np.float32).tolist()\n",
    "    query_embedding_array = array.array('f', query_embedding)\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        # Enable runtime stats if needed\n",
    "        if show_explain:\n",
    "            cur.execute(\"ALTER SESSION SET statistics_level = ALL\")\n",
    "\n",
    "        # --- Step 2: Hybrid query (Oracle Text + Vector) ---\n",
    "        sql = f\"\"\"\n",
    "            SELECT {\"/*+ GATHER_PLAN_STATISTICS */\" if show_explain else \"\"}\n",
    "                arxiv_id,\n",
    "                title,\n",
    "                abstract,\n",
    "                SUBSTR(text, 1, 200) AS text_snippet,\n",
    "                ROUND(1 - VECTOR_DISTANCE(embedding, :q, COSINE), 4) AS similarity_score\n",
    "            FROM research_papers\n",
    "            WHERE CONTAINS(text, :kw, 1) > 0\n",
    "            ORDER BY similarity_score DESC\n",
    "            FETCH APPROX FIRST {top_k} ROWS ONLY WITH TARGET ACCURACY 90\n",
    "        \"\"\"\n",
    "\n",
    "        cur.execute(sql, q=query_embedding_array, kw=search_phrase)\n",
    "        rows = cur.fetchall()\n",
    "        columns = [desc[0] for desc in cur.description]\n",
    "\n",
    "    # --- Step 3: Execution plan (optional) ---\n",
    "    exec_plan_text = None\n",
    "    if show_explain:\n",
    "        with conn.cursor() as cur_plan:\n",
    "            cur_plan.execute(\"\"\"\n",
    "                SELECT plan_table_output\n",
    "                FROM TABLE(DBMS_XPLAN.DISPLAY_CURSOR(NULL, NULL, 'ALLSTATS LAST +PREDICATE'))\n",
    "            \"\"\")\n",
    "            exec_plan_text = \"\\n\".join(r[0] for r in cur_plan.fetchall())\n",
    "\n",
    "        print(\"\\n====== Execution Plan (DBMS_XPLAN.DISPLAY_CURSOR) ======\")\n",
    "        print(exec_plan_text)\n",
    "        print(\"========================================================\\n\")\n",
    "\n",
    "    return rows, columns, exec_plan_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91ba38db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Hybrid Search: 'optimization'\n",
      "üìä Found 3 results\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARXIV_ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>TEXT_SNIPPET</th>\n",
       "      <th>SIMILARITY_SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0912.1394</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>This note suggests that near earth objects and...</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>0.7392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0901.0846</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>OGLE III and MOA II are discovering 600-1000 G...</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>0.6409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0904.4106</td>\n",
       "      <td>Is the HR 8799 extrasolar system destined for ...</td>\n",
       "      <td>The recent discovery of a three-planet extraso...</td>\n",
       "      <td>Is the HR 8799 extrasolar system destined for ...</td>\n",
       "      <td>0.5576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ARXIV_ID                                              TITLE  \\\n",
       "0  0912.1394  Are Near Earth Objects the Key to Optimization...   \n",
       "1  0901.0846  A Metric and Optimisation Scheme for Microlens...   \n",
       "2  0904.4106  Is the HR 8799 extrasolar system destined for ...   \n",
       "\n",
       "                                            ABSTRACT  \\\n",
       "0  This note suggests that near earth objects and...   \n",
       "1  OGLE III and MOA II are discovering 600-1000 G...   \n",
       "2  The recent discovery of a three-planet extraso...   \n",
       "\n",
       "                                        TEXT_SNIPPET  SIMILARITY_SCORE  \n",
       "0  Are Near Earth Objects the Key to Optimization...            0.7392  \n",
       "1  A Metric and Optimisation Scheme for Microlens...            0.6409  \n",
       "2  Is the HR 8799 extrasolar system destined for ...            0.5576  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows, columns, exec_plan = hybrid_search_research_papers_pre_filter(\n",
    "    conn,\n",
    "    embedding_model,\n",
    "    search_phrase=SEARCH_TEXT_KEYWORDS,\n",
    "    top_k=10,\n",
    "    show_explain=False\n",
    ")\n",
    "\n",
    "pre_filter_results_df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "print(f\"üîç Hybrid Search: '{SEARCH_TEXT_KEYWORDS}'\")\n",
    "print(f\"üìä Found {len(pre_filter_results_df)} results\\n\")\n",
    "\n",
    "pre_filter_results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e5fda2",
   "metadata": {},
   "source": [
    "#### 4.3.2 Post Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae572091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import array\n",
    "import numpy as np\n",
    "\n",
    "def hybrid_search_research_papers_postfilter(\n",
    "    conn,\n",
    "    embedding_model,\n",
    "    search_phrase: str,\n",
    "    top_k: int = 10,\n",
    "    candidate_k: int = 200,\n",
    "    show_explain: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform a hybrid search using Vector Search first, then Oracle Text filtering.\n",
    "    Returns top results ranked by semantic similarity but filtered by lexical match.\n",
    "\n",
    "    Args:\n",
    "        conn: Oracle database connection object.\n",
    "        embedding_model: Model with `.encode()` method (e.g., SentenceTransformer).\n",
    "        search_phrase (str): Search phrase used for both embedding and text filtering.\n",
    "        top_k (int): Number of top results to return (default = 10).\n",
    "        candidate_k (int): Number of initial vector candidates (default = 200).\n",
    "        show_explain (bool): If True, prints the execution plan.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (rows, columns, exec_plan_text or None)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Step 1: Encode search phrase into a normalized query vector ---\n",
    "    query_embedding = embedding_model.encode(\n",
    "        [f\"search_query: {search_phrase}\"],\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    )[0].astype(np.float32).tolist()\n",
    "    query_embedding_array = array.array('f', query_embedding)\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        # Enable runtime statistics if requested\n",
    "        if show_explain:\n",
    "            cur.execute(\"ALTER SESSION SET statistics_level = ALL\")\n",
    "\n",
    "        # --- Step 2: Hybrid query (Vector first ‚Üí Text filter) ---\n",
    "        sql = f\"\"\"\n",
    "            WITH vec_candidates AS (\n",
    "                SELECT\n",
    "                    arxiv_id,\n",
    "                    title,\n",
    "                    abstract,\n",
    "                    text,\n",
    "                    1 - VECTOR_DISTANCE(embedding, :q, COSINE) AS similarity_score\n",
    "                FROM research_papers\n",
    "                ORDER BY similarity_score DESC\n",
    "                FETCH APPROX FIRST {candidate_k} ROWS ONLY WITH TARGET ACCURACY 90\n",
    "            )\n",
    "            SELECT {\"/*+ GATHER_PLAN_STATISTICS */\" if show_explain else \"\"}\n",
    "                arxiv_id,\n",
    "                title,\n",
    "                SUBSTR(text, 1, 200) AS text_snippet,\n",
    "                ROUND(similarity_score, 4) AS similarity_score\n",
    "            FROM vec_candidates\n",
    "            WHERE CONTAINS(text, :kw, 1) > 0\n",
    "            ORDER BY similarity_score DESC\n",
    "            FETCH FIRST {top_k} ROWS ONLY\n",
    "        \"\"\"\n",
    "\n",
    "        cur.execute(sql, q=query_embedding_array, kw=search_phrase)\n",
    "        rows = cur.fetchall()\n",
    "        columns = [desc[0] for desc in cur.description]\n",
    "\n",
    "    # --- Step 3: Fetch and display execution plan (optional) ---\n",
    "    exec_plan_text = None\n",
    "    if show_explain:\n",
    "        with conn.cursor() as cur_plan:\n",
    "            cur_plan.execute(\"\"\"\n",
    "                SELECT plan_table_output\n",
    "                FROM TABLE(DBMS_XPLAN.DISPLAY_CURSOR(NULL, NULL, 'ALLSTATS LAST +PREDICATE'))\n",
    "            \"\"\")\n",
    "            exec_plan_text = \"\\n\".join(r[0] for r in cur_plan.fetchall())\n",
    "\n",
    "        print(\"\\n====== Execution Plan (DBMS_XPLAN.DISPLAY_CURSOR) ======\")\n",
    "        print(exec_plan_text)\n",
    "        print(\"========================================================\\n\")\n",
    "\n",
    "    return rows, columns, exec_plan_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4048cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Hybrid Search: 'optimization'\n",
      "üìä Found 3 results\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARXIV_ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>TEXT_SNIPPET</th>\n",
       "      <th>SIMILARITY_SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0912.1394</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>0.7392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0901.0846</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>0.6409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0904.4106</td>\n",
       "      <td>Is the HR 8799 extrasolar system destined for ...</td>\n",
       "      <td>Is the HR 8799 extrasolar system destined for ...</td>\n",
       "      <td>0.5576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ARXIV_ID                                              TITLE  \\\n",
       "0  0912.1394  Are Near Earth Objects the Key to Optimization...   \n",
       "1  0901.0846  A Metric and Optimisation Scheme for Microlens...   \n",
       "2  0904.4106  Is the HR 8799 extrasolar system destined for ...   \n",
       "\n",
       "                                        TEXT_SNIPPET  SIMILARITY_SCORE  \n",
       "0  Are Near Earth Objects the Key to Optimization...            0.7392  \n",
       "1  A Metric and Optimisation Scheme for Microlens...            0.6409  \n",
       "2  Is the HR 8799 extrasolar system destined for ...            0.5576  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows, columns, exec_plan = hybrid_search_research_papers_postfilter(\n",
    "    conn,\n",
    "    embedding_model,\n",
    "    search_phrase=SEARCH_TEXT_KEYWORDS,\n",
    "    top_k=10,\n",
    "    show_explain=False\n",
    ")\n",
    "\n",
    "post_filter_results_df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "print(f\"üîç Hybrid Search: '{SEARCH_TEXT_KEYWORDS}'\")\n",
    "print(f\"üìä Found {len(post_filter_results_df)} results\\n\")\n",
    "\n",
    "post_filter_results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ba61b5",
   "metadata": {},
   "source": [
    "Observe pre/post filtering technques in a table side by side "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5e521f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARXIV_ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>TEXT_SNIPPET</th>\n",
       "      <th>SIMILARITY_SCORE</th>\n",
       "      <th>ARXIV_ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>TEXT_SNIPPET</th>\n",
       "      <th>SIMILARITY_SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0912.1394</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>This note suggests that near earth objects and...</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>0.7392</td>\n",
       "      <td>0912.1394</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>0.7392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0901.0846</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>OGLE III and MOA II are discovering 600-1000 G...</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>0.6409</td>\n",
       "      <td>0901.0846</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>0.6409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0904.4106</td>\n",
       "      <td>Is the HR 8799 extrasolar system destined for ...</td>\n",
       "      <td>The recent discovery of a three-planet extraso...</td>\n",
       "      <td>Is the HR 8799 extrasolar system destined for ...</td>\n",
       "      <td>0.5576</td>\n",
       "      <td>0904.4106</td>\n",
       "      <td>Is the HR 8799 extrasolar system destined for ...</td>\n",
       "      <td>Is the HR 8799 extrasolar system destined for ...</td>\n",
       "      <td>0.5576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ARXIV_ID                                              TITLE  \\\n",
       "0  0912.1394  Are Near Earth Objects the Key to Optimization...   \n",
       "1  0901.0846  A Metric and Optimisation Scheme for Microlens...   \n",
       "2  0904.4106  Is the HR 8799 extrasolar system destined for ...   \n",
       "\n",
       "                                            ABSTRACT  \\\n",
       "0  This note suggests that near earth objects and...   \n",
       "1  OGLE III and MOA II are discovering 600-1000 G...   \n",
       "2  The recent discovery of a three-planet extraso...   \n",
       "\n",
       "                                        TEXT_SNIPPET  SIMILARITY_SCORE  \\\n",
       "0  Are Near Earth Objects the Key to Optimization...            0.7392   \n",
       "1  A Metric and Optimisation Scheme for Microlens...            0.6409   \n",
       "2  Is the HR 8799 extrasolar system destined for ...            0.5576   \n",
       "\n",
       "    ARXIV_ID                                              TITLE  \\\n",
       "0  0912.1394  Are Near Earth Objects the Key to Optimization...   \n",
       "1  0901.0846  A Metric and Optimisation Scheme for Microlens...   \n",
       "2  0904.4106  Is the HR 8799 extrasolar system destined for ...   \n",
       "\n",
       "                                        TEXT_SNIPPET  SIMILARITY_SCORE  \n",
       "0  Are Near Earth Objects the Key to Optimization...            0.7392  \n",
       "1  A Metric and Optimisation Scheme for Microlens...            0.6409  \n",
       "2  Is the HR 8799 extrasolar system destined for ...            0.5576  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the results side by side\n",
    "pd.concat([pre_filter_results_df, post_filter_results_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b041cd8b",
   "metadata": {},
   "source": [
    "| Approach                          | Strength                            | Best For                                |\n",
    "| --------------------------------- | ----------------------------------- | --------------------------------------- |\n",
    "| **Pre-filter** (`CONTAINS` first) | Fast, keyword-strict                | Narrow keyword search                   |\n",
    "| **Post-filter** (this one)        | Semantically rich but still precise | Broader exploratory or research queries |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a008720",
   "metadata": {},
   "source": [
    "#### 4.3.3 Reciprocial Rank Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7aefbf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import array\n",
    "import numpy as np\n",
    "import oracledb\n",
    "\n",
    "def hybrid_rrf_search(\n",
    "    conn,\n",
    "    embedding_model,\n",
    "    search_phrase: str,\n",
    "    top_k: int = 10,\n",
    "    per_list: int = 120,     # candidates from each list before fusion (>= 10x top_k is a good rule)\n",
    "    k: int = 60,             # RRF smoothing constant (60 is standard)\n",
    "    phrase_safe: bool = True,\n",
    "    show_explain: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Local-friendly RRF fusion of Vector + Oracle Text results on research_papers(text, embedding).\n",
    "\n",
    "    Prereqs (local/Docker Free OK):\n",
    "      - VECTOR column/index on research_papers(embedding)  -- IVF/HNSW\n",
    "      - Oracle Text index on research_papers(text)         -- e.g. CREATE SEARCH INDEX rp_text_idx ON research_papers(text);\n",
    "\n",
    "    RRF = 1/(k + r_vec) + 1/(k + r_txt), where r_vec and r_txt are ranks (1 = best).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Encode query for vector modality (align with your doc prefixing scheme)\n",
    "    qv = embedding_model.encode(\n",
    "        [f\"search_query: {search_phrase}\"],\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    )[0].astype(np.float32).tolist()\n",
    "    qv = array.array('f', qv)\n",
    "\n",
    "    # 2) Phrase-safe text query for Oracle Text (optional)\n",
    "    kw = f\"\\\"{search_phrase}\\\"\" if (phrase_safe and \" \" in search_phrase.strip()) else search_phrase\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        if show_explain:\n",
    "            cur.execute(\"ALTER SESSION SET statistics_level = ALL\")\n",
    "\n",
    "        sql = f\"\"\"\n",
    "            WITH\n",
    "            /* Vector top-N with ranks (higher similarity first) */\n",
    "            vec AS (\n",
    "              SELECT\n",
    "                arxiv_id,\n",
    "                title,\n",
    "                SUBSTR(text, 1, 200) AS text_snippet,\n",
    "                1 - VECTOR_DISTANCE(embedding, :q, COSINE) AS sim_vec,\n",
    "                ROW_NUMBER() OVER (ORDER BY 1 - VECTOR_DISTANCE(embedding, :q, COSINE) DESC) AS r_vec\n",
    "              FROM research_papers\n",
    "              FETCH APPROX FIRST {per_list} ROWS ONLY WITH TARGET ACCURACY 90\n",
    "            ),\n",
    "            /* Oracle Text top-N with ranks (higher SCORE(1) first) */\n",
    "            txt AS (\n",
    "              SELECT\n",
    "                arxiv_id,\n",
    "                title,\n",
    "                SUBSTR(text, 1, 200) AS text_snippet,\n",
    "                SCORE(1) AS score_txt,\n",
    "                ROW_NUMBER() OVER (ORDER BY SCORE(1) DESC) AS r_txt\n",
    "              FROM research_papers\n",
    "              WHERE CONTAINS(text, :kw, 1) > 0\n",
    "              FETCH FIRST {per_list} ROWS ONLY\n",
    "            ),\n",
    "            /* Fuse by arxiv_id; keep docs present in either list */\n",
    "            fused AS (\n",
    "              SELECT\n",
    "                COALESCE(v.arxiv_id, t.arxiv_id)           AS arxiv_id,\n",
    "                COALESCE(v.title,     t.title)             AS title,\n",
    "                COALESCE(v.text_snippet, t.text_snippet)   AS text_snippet,\n",
    "                NVL(v.r_vec,  999999) AS r_vec,\n",
    "                NVL(t.r_txt,  999999) AS r_txt,\n",
    "                NVL(v.sim_vec, 0)     AS sim_vec,\n",
    "                NVL(t.score_txt, 0)   AS score_txt\n",
    "              FROM vec v\n",
    "              FULL OUTER JOIN txt t\n",
    "                ON t.arxiv_id = v.arxiv_id\n",
    "            )\n",
    "            SELECT {\"/*+ GATHER_PLAN_STATISTICS */\" if show_explain else \"\"}\n",
    "              arxiv_id,\n",
    "              title,\n",
    "              text_snippet,\n",
    "              ROUND( (1.0/(:k + r_vec)) + (1.0/(:k + r_txt)), 6 ) AS rrf_score,\n",
    "              r_vec,\n",
    "              r_txt,\n",
    "              ROUND(sim_vec, 4)  AS sim_vec,\n",
    "              ROUND(score_txt,4) AS score_txt\n",
    "            FROM fused\n",
    "            ORDER BY rrf_score DESC, title\n",
    "            FETCH FIRST {top_k} ROWS ONLY\n",
    "        \"\"\"\n",
    "\n",
    "        cur.execute(sql, q=qv, kw=kw, k=k)\n",
    "        rows = cur.fetchall()\n",
    "        columns = [d[0] for d in cur.description]\n",
    "\n",
    "    exec_plan_text = None\n",
    "    if show_explain:\n",
    "        with conn.cursor() as cur_plan:\n",
    "            cur_plan.execute(\"\"\"\n",
    "                SELECT plan_table_output\n",
    "                FROM TABLE(DBMS_XPLAN.DISPLAY_CURSOR(NULL, NULL, 'ALLSTATS LAST +PREDICATE'))\n",
    "            \"\"\")\n",
    "            exec_plan_text = \"\\n\".join(r[0] for r in cur_plan.fetchall())\n",
    "            print(\"\\n====== Execution Plan (DBMS_XPLAN.DISPLAY_CURSOR) ======\")\n",
    "            print(exec_plan_text)\n",
    "            print(\"========================================================\\n\")\n",
    "\n",
    "    return rows, columns, exec_plan_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "128e687c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Hybrid Search: 'optimization'\n",
      "üìä Found 3 results\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARXIV_ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>TEXT_SNIPPET</th>\n",
       "      <th>RRF_SCORE</th>\n",
       "      <th>R_VEC</th>\n",
       "      <th>R_TXT</th>\n",
       "      <th>SIM_VEC</th>\n",
       "      <th>SCORE_TXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0912.1394</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>0.032787</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7392</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0901.0846</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>0.032002</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6409</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0904.4106</td>\n",
       "      <td>Is the HR 8799 extrasolar system destined for ...</td>\n",
       "      <td>Is the HR 8799 extrasolar system destined for ...</td>\n",
       "      <td>0.021811</td>\n",
       "      <td>116</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5576</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ARXIV_ID                                              TITLE  \\\n",
       "0  0912.1394  Are Near Earth Objects the Key to Optimization...   \n",
       "1  0901.0846  A Metric and Optimisation Scheme for Microlens...   \n",
       "2  0904.4106  Is the HR 8799 extrasolar system destined for ...   \n",
       "\n",
       "                                        TEXT_SNIPPET  RRF_SCORE  R_VEC  R_TXT  \\\n",
       "0  Are Near Earth Objects the Key to Optimization...   0.032787      1      1   \n",
       "1  A Metric and Optimisation Scheme for Microlens...   0.032002      2      3   \n",
       "2  Is the HR 8799 extrasolar system destined for ...   0.021811    116      2   \n",
       "\n",
       "   SIM_VEC  SCORE_TXT  \n",
       "0   0.7392         42  \n",
       "1   0.6409         11  \n",
       "2   0.5576         11  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows, columns, exec_plan = hybrid_rrf_search(\n",
    "    conn,\n",
    "    embedding_model,\n",
    "    search_phrase=SEARCH_TEXT_KEYWORDS,\n",
    "    top_k=3,\n",
    "    show_explain=False\n",
    ")\n",
    "\n",
    "rrf_results_df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "print(f\"üîç Hybrid Search: '{SEARCH_TEXT_KEYWORDS}'\")\n",
    "print(f\"üìä Found {len(rrf_results_df)} results\\n\")\n",
    "\n",
    "rrf_results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0832de",
   "metadata": {},
   "source": [
    "# Part 5: Building a RAG pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "741e36c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# Function to securely get and set environment variables\n",
    "def set_env_securely(var_name, prompt):\n",
    "    value = getpass.getpass(prompt)\n",
    "    os.environ[var_name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1cf5406",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_env_securely(\"OPENAI_API_KEY\", \"Enter your OPEN API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f41960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the OpenAI Python client library\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Initialize the OpenAI client (API key read from env var OPENAI_API_KEY)\n",
    "openai_client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# Create a Response API request (using the Responses API)\n",
    "\n",
    "# Use the Responses API\n",
    "response = openai_client.responses.create(\n",
    "    model=\"gpt-4o\",              # specify model\n",
    "    input=\"Hello! I‚Äôm a user!\",  # user message as a text input\n",
    "    instructions=\"You are a research paper assistant.\",  # assistant role/instruction\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "27805348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you with your research today?\n"
     ]
    }
   ],
   "source": [
    "# Print the output text\n",
    "print(response.output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "005a01c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_paper_assistant_rag_pipeline(\n",
    "    conn,\n",
    "    embedding_model,\n",
    "    user_query: str,\n",
    "    top_k: int = 10,\n",
    "    retrieval_mode: str = \"hybrid\",\n",
    "    show_explain: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Research Paper Assistant ‚Äî Retrieval-Augmented Generation (RAG) pipeline\n",
    "    built on SQL-based retrieval functions and powered by the OpenAI Responses API.\n",
    "\n",
    "    Retrieval techniques available:\n",
    "        - 'keyword'  ‚Üí uses keyword_search_research_papers()\n",
    "        - 'vector'   ‚Üí uses vector_search_research_papers()\n",
    "        - 'hybrid'   ‚Üí uses hybrid_search_research_papers() [default]\n",
    "\n",
    "    Args:\n",
    "        conn: Oracle database connection.\n",
    "        embedding_model: Embedding model (e.g., SentenceTransformer, Voyage).\n",
    "        user_query (str): Research question from the user.\n",
    "        top_k (int): Number of top documents to retrieve.\n",
    "        retrieval_mode (str): Retrieval method ('keyword', 'vector', 'hybrid').\n",
    "        show_explain (bool): Whether to show the SQL execution plan.\n",
    "\n",
    "    Returns:\n",
    "        str: LLM-generated research synthesis with citations.\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 1. Retrieve relevant research papers using the selected retrieval mode\n",
    "    # ----------------------------------------------------------------------\n",
    "    if retrieval_mode == \"keyword\":\n",
    "        rows, columns = keyword_search_research_papers(conn, user_query)\n",
    "        exec_plan_text = None\n",
    "\n",
    "    elif retrieval_mode == \"vector\":\n",
    "        rows, columns = vector_search_research_papers(conn, embedding_model, user_query, top_k)\n",
    "        exec_plan_text = None\n",
    "\n",
    "    else:  # default: hybrid retrieval\n",
    "        rows, columns, exec_plan_text = hybrid_search_research_papers_pre_filter(\n",
    "            conn=conn,\n",
    "            embedding_model=embedding_model,\n",
    "            search_phrase=user_query,\n",
    "            top_k=top_k,\n",
    "            show_explain=show_explain\n",
    "        )\n",
    "\n",
    "    retrieved_count = len(rows) if rows else 0\n",
    "    print(f\"üìä Retrieved {retrieved_count} papers using {retrieval_mode.upper()} retrieval.\")\n",
    "\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 2. Convert retrieved rows to formatted LLM context\n",
    "    # ----------------------------------------------------------------------\n",
    "    formatted_context = \"\"\n",
    "    if retrieved_count > 0:\n",
    "        formatted_context += f\"\\n\\nüìö {retrieved_count} relevant research papers retrieved:\\n\\n\"\n",
    "        for i, row in enumerate(rows):\n",
    "            row_data = dict(zip(columns, row))\n",
    "            title = row_data.get(\"TITLE\", \"Untitled Paper\")\n",
    "            abstract = row_data.get(\"ABSTRACT\", \"No abstract available.\")\n",
    "            snippet = row_data.get(\"TEXT_SNIPPET\", \"\")\n",
    "            score = (\n",
    "                row_data.get(\"SIMILARITY_SCORE\")\n",
    "                or row_data.get(\"TEXT_RELEVANCE_SCORE\")\n",
    "                or \"N/A\"\n",
    "            )\n",
    "            formatted_context += (\n",
    "                f\"[{i+1}] **{title}**\\n\"\n",
    "                f\"Abstract: {abstract}\\n\"\n",
    "                f\"Snippet: {snippet}\\n\"\n",
    "                f\"Relevance Score: {score}\\n\\n\"\n",
    "            )\n",
    "    else:\n",
    "        formatted_context = \"\\n\\n‚ö†Ô∏è No relevant papers were retrieved from the database.\\n\"\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 3. Construct the prompt for the Responses API\n",
    "    # ----------------------------------------------------------------------\n",
    "    prompt = f\"\"\"\n",
    "            You are a **Research Paper Assistant** that synthesizes academic literature to help answer user questions.\n",
    "\n",
    "            User Query: {user_query}\n",
    "\n",
    "            Number of retrieved papers: {retrieved_count}\n",
    "            {formatted_context}\n",
    "\n",
    "            Please:\n",
    "            - Summarize the findings most relevant to the query.\n",
    "            - Use citation numbers [X] to support claims.\n",
    "            - Highlight consensus, innovation, or research gaps.\n",
    "            - If there is insufficient context, clearly say so.\n",
    "            \"\"\"\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 4. Call the OpenAI Responses API\n",
    "    # ----------------------------------------------------------------------\n",
    "    response = openai_client.responses.create(\n",
    "        model=\"gpt-4o\",\n",
    "        input=prompt,\n",
    "        instructions=\"You are a scientific research assistant. Use only the provided context to answer. Always cite papers [1], [2], etc.\",\n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 5. Optionally print SQL execution plan (if hybrid)\n",
    "    # ----------------------------------------------------------------------\n",
    "    if show_explain and exec_plan_text:\n",
    "        print(\"\\n====== SQL Execution Plan ======\")\n",
    "        print(exec_plan_text)\n",
    "        print(\"================================\\n\")\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 6. Return the LLM‚Äôs output text\n",
    "    # ----------------------------------------------------------------------\n",
    "    return response.output_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f4b00b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Retrieved 3 papers using HYBRID retrieval.\n",
      "The retrieved papers discuss various aspects of optimization, each in a different context:\n",
      "\n",
      "1. **Central Force Optimization and Near Earth Objects**: Paper [1] explores a novel approach to deterministic optimization by drawing parallels between Near Earth Objects (NEOs) and Central Force Optimization (CFO). The study suggests that the oscillatory patterns observed in both NEOs and CFO could provide insights into overcoming challenges like local trapping and convergence in optimization problems. This represents an innovative intersection of astrophysics and optimization theory, potentially offering new methods for solving complex optimization issues.\n",
      "\n",
      "2. **Optimization in Microlens Planet Searches**: Paper [2] focuses on optimizing the search for planets using microlensing techniques. It proposes an automatic prioritization algorithm that considers various observational parameters to maximize the detection of planetary anomalies. This optimization scheme is designed to enhance the efficiency of microlens observations, highlighting a practical application of optimization in astronomy.\n",
      "\n",
      "3. **Stability of the HR 8799 Extrasolar System**: Paper [3] investigates the dynamical stability of the HR 8799 extrasolar system using an optimization algorithm that incorporates stability constraints. The study finds limited regions of stable orbits, suggesting that the system might undergo planetary scattering. This research demonstrates the application of optimization algorithms in studying celestial mechanics and system stability.\n",
      "\n",
      "**Consensus and Innovation**:\n",
      "- There is a consensus on the importance of optimization in both theoretical and applied contexts, ranging from astrophysical phenomena to practical observational strategies.\n",
      "- Papers [1] and [2] highlight innovative approaches by integrating optimization with astrophysical concepts and observational techniques, respectively.\n",
      "\n",
      "**Research Gaps**:\n",
      "- Paper [1] suggests a speculative link between NEOs and optimization, indicating a need for further research to validate this connection.\n",
      "- Paper [3] identifies potential instability in the HR 8799 system, pointing to a gap in understanding long-term planetary dynamics.\n",
      "\n",
      "Overall, these papers illustrate diverse applications and innovative approaches in optimization, with potential for further exploration and validation in both theoretical and practical domains.\n"
     ]
    }
   ],
   "source": [
    "summary = research_paper_assistant_rag_pipeline(\n",
    "    conn=conn,\n",
    "    embedding_model=embedding_model,\n",
    "    user_query=SEARCH_TEXT_KEYWORDS,\n",
    "    top_k=5,\n",
    "    retrieval_mode=\"hybrid\",  # options: 'keyword', 'vector', 'hybrid'\n",
    "    show_explain=False\n",
    ")\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104b31b8",
   "metadata": {},
   "source": [
    "# Part 6: AI Agents with OpenAI and Oracle AI Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "74defe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -Uq openai-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0d87a8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_MODEL = \"gpt-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1222a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner\n",
    "\n",
    "research_paper_assistant = Agent(\n",
    "    name=\"Research Paper Assistant\",\n",
    "    model=OPENAI_MODEL,\n",
    "    instructions=\"\"\"\n",
    "      You are a Research Paper Assistant focused on helping users explore, analyze, and summarize\n",
    "      academic research.\n",
    "\n",
    "      Maintain a professional, concise, and scholarly tone appropriate for research discussions.\n",
    "    \"\"\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a1ab0778",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_result = await Runner.run(\n",
    "    starting_agent=research_paper_assistant,\n",
    "    input=\"Summarize recent research on optimization techniques for planetary exploration.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c2eda5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization techniques for planetary exploration have been a significant aspect of recent research due to increased focus on space exploration. These techniques aim at maximizing the scientific return of missions while minimizing the associated cost, time, and risk factors.\n",
      "\n",
      "In \"Global Optimization for Spacecraft Planetary Capture Trajectories\" (2020) by Li et al., the researchers propose a novel bi-level global optimization method combining Radau pseudospectral method and niching genetic algorithm for optimal planetary capture trajectories. This strategy aims at minimising the energy consumption and providing an optimal path for the spacecraft.\n",
      "\n",
      "In \"Particle Swarm Optimization-Based Spacecraft Mission Planning for Planetary Exploration\" (2020) by Zhang and Li, a particle swarm optimization (PSO) framework was developed. It demonstrated the effectiveness of using swarms of unmanned spacecraft for exploration purposes. Their model increasingly minimizes the reliance on ground-based decision making, instead it makes use of crowd intelligence to optimize route planning and resource allocation.\n",
      "\n",
      "Moreover, a study by Englander et al. titled \"Planetary exploration mission planning with probabilistic temporal planning\" (2021) proposed a probabilistic temporal planning concept. It uses uncertainties related to mission execution, such as the timing of scientific observations and rover movements, to offer flexible and robust mission plans.\n",
      "\n",
      "Walker et al. in \"Optimal Control Strategies for Robotic Planetary Exploration\" (2019) discuss a model predictive control (MPC) method for effective exploration. This method generates dynamic instructions for rovers depending on the current status and trajectory of the mission. \n",
      "\n",
      "In conclusion, recent research in optimization techniques for planetary exploration focuses on developing strategies and algorithms that maximize scientific returns while minimizing costs and risks connected with space missions. These include global optimization for trajectories, PSO for mission planning, probabilistic temporal planning for execution uncertainties, and MPC for robotic exploration.\n"
     ]
    }
   ],
   "source": [
    "print(run_result.final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dd18f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.tool import function_tool\n",
    "\n",
    "@function_tool\n",
    "def get_research_papers(user_query: str, retrieval_mode: str = \"hybrid\", top_k: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves academic research papers relevant to the user's query.\n",
    "\n",
    "    This tool queries the research_papers SQL table using one of three retrieval techniques:\n",
    "        - 'keyword'  ‚Üí lexical search via LIKE filtering\n",
    "        - 'vector'   ‚Üí semantic similarity search\n",
    "        - 'hybrid'   ‚Üí combines keyword prefiltering + vector similarity (default)\n",
    "\n",
    "    Use this tool when analyzing or summarizing scientific literature.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): Research topic or question to search for.\n",
    "        retrieval_mode (str): 'keyword', 'vector', or 'hybrid'. Default is 'hybrid'.\n",
    "        top_k (int): Number of top papers to retrieve (default=5).\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted summary of the most relevant research papers.\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Perform retrieval using SQL-based functions (defined earlier)\n",
    "    # ------------------------------------------------------------------\n",
    "    if retrieval_mode == \"keyword\":\n",
    "        rows, columns = keyword_search_research_papers(conn, user_query)\n",
    "    elif retrieval_mode == \"vector\":\n",
    "        rows, columns = vector_search_research_papers(conn, embedding_model, user_query, top_k)\n",
    "    else:\n",
    "        rows, columns, _ = hybrid_search_research_papers_pre_filter(\n",
    "            conn=conn,\n",
    "            embedding_model=embedding_model,\n",
    "            search_phrase=user_query,\n",
    "            top_k=top_k,\n",
    "            show_explain=False\n",
    "        )\n",
    "\n",
    "    retrieved_count = len(rows) if rows else 0\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Format the output into a readable string\n",
    "    # ------------------------------------------------------------------\n",
    "    if retrieved_count == 0:\n",
    "        return f\"No research papers found related to '{user_query}'.\"\n",
    "\n",
    "    formatted_results = [f\"üìö {retrieved_count} papers retrieved for query: '{user_query}'\\n\"]\n",
    "    for i, row in enumerate(rows):\n",
    "        row_data = dict(zip(columns, row))\n",
    "        title = row_data.get(\"TITLE\", \"Untitled Paper\")\n",
    "        abstract = row_data.get(\"ABSTRACT\", \"No abstract available.\")\n",
    "        score = (\n",
    "            row_data.get(\"SIMILARITY_SCORE\")\n",
    "            or row_data.get(\"TEXT_RELEVANCE_SCORE\")\n",
    "            or \"N/A\"\n",
    "        )\n",
    "        formatted_results.append(\n",
    "            f\"[{i+1}] {title}\\n\"\n",
    "            f\"Abstract: {abstract}\\n\"\n",
    "            f\"Relevance Score: {score}\\n\"\n",
    "        )\n",
    "\n",
    "    return \"\\n\".join(formatted_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "625c3b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "research_paper_assistant.tools.append(get_research_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3bdca1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_result_with_tool = await Runner.run(\n",
    "    starting_agent=research_paper_assistant,\n",
    "    input=\"Get me information on rover navigation, planetary data collection, mission planning, resource allocation, or other related fields\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "92279dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I found a research paper that seems relevant to your query. \n",
      "\n",
      "**Title:** The nucleus of 103P/Hartley 2, target of the EPOXI mission\n",
      "\n",
      "**Abstract:** This research is about 103P/Hartley 2, which was chosen as the target comet for the Deep Impact extended mission, EPOXI, in October 2007. Notably, there have been no direct optical observations of the nucleus of this comet, as it has always been highly active when previously observed. The paper's goals were to confirm that the comet had not broken up and was in the predicted position, providing astrometry and brightness information for mission planning, and carrying on with the nucleus' characterization. The comet was observed at heliocentric distances between 5.7 and 5.5 AU, using FORS2 at the VLT, on four occasions between May and July 2008. VRI photometry was performed on deep stacked images to monitor activity, measure the absolute magnitude, and therefore estimate the size of the nucleus. \n",
      "\n",
      "**Relevance Score:** 0.6027\n",
      "\n",
      "The authors highlight that the comet did not have a visible coma, but there was faint activity, possibly from the dust trail from previous activity. This activity seems to fade at later epochs, implying that this is a continuation of activity past aphelion from the previous apparition rather than an early start to activity before the next perihelion. The data imply a nucleus radius of less than or equal to 1 km for an assumed 4% albedo; the authors estimate a ~6% albedo. They measure a color of (V-R) = 0.26 ¬± 0.09.\n",
      "\n",
      "Please note that while this paper may not touch all aspects of your query, it does provide valuable insights in the fields of astrometry and mission planning.\n"
     ]
    }
   ],
   "source": [
    "print(run_result_with_tool.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9afe21b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ModelResponse(output=[ResponseFunctionToolCall(arguments='{\\n  \"user_query\": \"rover navigation, planetary data collection, mission planning, resource allocation\",\\n  \"retrieval_mode\": \"hybrid\",\\n  \"top_k\": 5\\n}', call_id='call_kViqZyB9TktfHGt2q54nAgG6', name='get_research_papers', type='function_call', id='fc_02a2a96ae08281ac00690e526031dc81949fcfb533b6283e66', status='completed')],\n",
      "               usage=Usage(requests=1,\n",
      "                           input_tokens=200,\n",
      "                           input_tokens_details=InputTokensDetails(cached_tokens=0),\n",
      "                           output_tokens=51,\n",
      "                           output_tokens_details=OutputTokensDetails(reasoning_tokens=0),\n",
      "                           total_tokens=251,\n",
      "                           request_usage_entries=[]),\n",
      "               response_id='resp_02a2a96ae08281ac00690e525e6da881948478d6ad9e49f2fd'),\n",
      " ModelResponse(output=[ResponseOutputMessage(id='msg_02a2a96ae08281ac00690e5268a8c08194919d2b113562425c', content=[ResponseOutputText(annotations=[], text=\"I found a research paper that seems relevant to your query. \\n\\n**Title:** The nucleus of 103P/Hartley 2, target of the EPOXI mission\\n\\n**Abstract:** This research is about 103P/Hartley 2, which was chosen as the target comet for the Deep Impact extended mission, EPOXI, in October 2007. Notably, there have been no direct optical observations of the nucleus of this comet, as it has always been highly active when previously observed. The paper's goals were to confirm that the comet had not broken up and was in the predicted position, providing astrometry and brightness information for mission planning, and carrying on with the nucleus' characterization. The comet was observed at heliocentric distances between 5.7 and 5.5 AU, using FORS2 at the VLT, on four occasions between May and July 2008. VRI photometry was performed on deep stacked images to monitor activity, measure the absolute magnitude, and therefore estimate the size of the nucleus. \\n\\n**Relevance Score:** 0.6027\\n\\nThe authors highlight that the comet did not have a visible coma, but there was faint activity, possibly from the dust trail from previous activity. This activity seems to fade at later epochs, implying that this is a continuation of activity past aphelion from the previous apparition rather than an early start to activity before the next perihelion. The data imply a nucleus radius of less than or equal to 1 km for an assumed 4% albedo; the authors estimate a ~6% albedo. They measure a color of (V-R) = 0.26 ¬± 0.09.\\n\\nPlease note that while this paper may not touch all aspects of your query, it does provide valuable insights in the fields of astrometry and mission planning.\", type='output_text', logprobs=[])], role='assistant', status='completed', type='message')],\n",
      "               usage=Usage(requests=1,\n",
      "                           input_tokens=665,\n",
      "                           input_tokens_details=InputTokensDetails(cached_tokens=0),\n",
      "                           output_tokens=382,\n",
      "                           output_tokens_details=OutputTokensDetails(reasoning_tokens=0),\n",
      "                           total_tokens=1047,\n",
      "                           request_usage_entries=[]),\n",
      "               response_id='resp_02a2a96ae08281ac00690e52678be88194935dd4d91ac982d6')]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(run_result_with_tool.raw_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31a451b",
   "metadata": {},
   "source": [
    "### Build an Agent with Multiple Tool Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "785c0266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.tool import function_tool\n",
    "\n",
    "@function_tool\n",
    "def get_past_research_conversations(user_query: str, top_k: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves relevant past research-related conversations or analyses related to the query.\n",
    "\n",
    "    This tool searches a SQL database of prior research assistant conversations, \n",
    "    literature discussions, or synthesis sessions to find relevant context. \n",
    "    It allows the research assistant to recall previous analyses or summaries \n",
    "    that addressed similar topics, providing continuity and richer insights.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The research topic, concept, or question to search for.\n",
    "        top_k (int): Number of top past discussions to retrieve (default=5).\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted examples of relevant past research discussions.\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Perform retrieval using the SQL-based hybrid search (vector + keyword)\n",
    "    # ------------------------------------------------------------------\n",
    "    rows, columns, _ = hybrid_search_research_papers_pre_filter(\n",
    "        conn=conn,\n",
    "        embedding_model=embedding_model,\n",
    "        search_phrase=user_query,\n",
    "        top_k=top_k,\n",
    "        show_explain=False\n",
    "    )\n",
    "\n",
    "    retrieved_count = len(rows) if rows else 0\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Format results for readability\n",
    "    # ------------------------------------------------------------------\n",
    "    if retrieved_count == 0:\n",
    "        return f\"No past research discussions found related to '{user_query}'.\"\n",
    "\n",
    "    formatted_results = [f\"üß† {retrieved_count} past research discussions retrieved for query: '{user_query}'\\n\"]\n",
    "    for i, row in enumerate(rows):\n",
    "        row_data = dict(zip(columns, row))\n",
    "        title = row_data.get(\"TITLE\", \"Untitled Discussion\")\n",
    "        abstract = row_data.get(\"ABSTRACT\", \"No summary available.\")\n",
    "        snippet = row_data.get(\"TEXT_SNIPPET\", \"\")\n",
    "        score = (\n",
    "            row_data.get(\"SIMILARITY_SCORE\")\n",
    "            or row_data.get(\"TEXT_RELEVANCE_SCORE\")\n",
    "            or \"N/A\"\n",
    "        )\n",
    "        formatted_results.append(\n",
    "            f\"[{i+1}] **{title}**\\n\"\n",
    "            f\"Summary: {abstract}\\n\"\n",
    "            f\"Snippet: {snippet}\\n\"\n",
    "            f\"Relevance Score: {score}\\n\"\n",
    "        )\n",
    "\n",
    "    return \"\\n\".join(formatted_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89db0e96",
   "metadata": {},
   "source": [
    "Let's update our agent instruction to ensure it knows when to utilize the right tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6f602932",
   "metadata": {},
   "outputs": [],
   "source": [
    "upgraded_research_paper_assistant = Agent(\n",
    "    name=\"Research Paper Assistant\",\n",
    "    model=OPENAI_MODEL,\n",
    "    instructions=\"\"\"\n",
    "    Always maintain an academic, evidence-based tone.\n",
    "    Your purpose is to help users explore, synthesize, and connect research insights ‚Äî\n",
    "    not to speculate or fabricate information.\n",
    "    \"\"\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fecf68a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach research retrieval tools to the upgraded research assistant\n",
    "upgraded_research_paper_assistant.tools.append(get_research_papers)\n",
    "upgraded_research_paper_assistant.tools.append(get_past_research_conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a66d1b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FunctionTool(name='get_research_papers',\n",
      "              description='Retrieves academic research papers relevant to the '\n",
      "                          \"user's query.\",\n",
      "              params_json_schema={'additionalProperties': False,\n",
      "                                  'properties': {'retrieval_mode': {'default': 'hybrid',\n",
      "                                                                    'description': \"'keyword', \"\n",
      "                                                                                   \"'vector', \"\n",
      "                                                                                   'or '\n",
      "                                                                                   \"'hybrid'. \"\n",
      "                                                                                   'Default '\n",
      "                                                                                   'is '\n",
      "                                                                                   \"'hybrid'.\",\n",
      "                                                                    'title': 'Retrieval '\n",
      "                                                                             'Mode',\n",
      "                                                                    'type': 'string'},\n",
      "                                                 'top_k': {'default': 5,\n",
      "                                                           'description': 'Number '\n",
      "                                                                          'of '\n",
      "                                                                          'top '\n",
      "                                                                          'papers '\n",
      "                                                                          'to '\n",
      "                                                                          'retrieve '\n",
      "                                                                          '(default=5).',\n",
      "                                                           'title': 'Top K',\n",
      "                                                           'type': 'integer'},\n",
      "                                                 'user_query': {'description': 'Research '\n",
      "                                                                               'topic '\n",
      "                                                                               'or '\n",
      "                                                                               'question '\n",
      "                                                                               'to '\n",
      "                                                                               'search '\n",
      "                                                                               'for.',\n",
      "                                                                'title': 'User '\n",
      "                                                                         'Query',\n",
      "                                                                'type': 'string'}},\n",
      "                                  'required': ['user_query',\n",
      "                                               'retrieval_mode',\n",
      "                                               'top_k'],\n",
      "                                  'title': 'get_research_papers_args',\n",
      "                                  'type': 'object'},\n",
      "              on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x15d5d4fe0>,\n",
      "              strict_json_schema=True,\n",
      "              is_enabled=True,\n",
      "              tool_input_guardrails=None,\n",
      "              tool_output_guardrails=None),\n",
      " FunctionTool(name='get_past_research_conversations',\n",
      "              description='Retrieves relevant past research-related '\n",
      "                          'conversations or analyses related to the query.\\n'\n",
      "                          '\\n'\n",
      "                          'This tool searches a SQL database of prior research '\n",
      "                          'assistant conversations, \\n'\n",
      "                          'literature discussions, or synthesis sessions to '\n",
      "                          'find relevant context. \\n'\n",
      "                          'It allows the research assistant to recall previous '\n",
      "                          'analyses or summaries \\n'\n",
      "                          'that addressed similar topics, providing continuity '\n",
      "                          'and richer insights.',\n",
      "              params_json_schema={'additionalProperties': False,\n",
      "                                  'properties': {'top_k': {'default': 5,\n",
      "                                                           'description': 'Number '\n",
      "                                                                          'of '\n",
      "                                                                          'top '\n",
      "                                                                          'past '\n",
      "                                                                          'discussions '\n",
      "                                                                          'to '\n",
      "                                                                          'retrieve '\n",
      "                                                                          '(default=5).',\n",
      "                                                           'title': 'Top K',\n",
      "                                                           'type': 'integer'},\n",
      "                                                 'user_query': {'description': 'The '\n",
      "                                                                               'research '\n",
      "                                                                               'topic, '\n",
      "                                                                               'concept, '\n",
      "                                                                               'or '\n",
      "                                                                               'question '\n",
      "                                                                               'to '\n",
      "                                                                               'search '\n",
      "                                                                               'for.',\n",
      "                                                                'title': 'User '\n",
      "                                                                         'Query',\n",
      "                                                                'type': 'string'}},\n",
      "                                  'required': ['user_query', 'top_k'],\n",
      "                                  'title': 'get_past_research_conversations_args',\n",
      "                                  'type': 'object'},\n",
      "              on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x150eb8040>,\n",
      "              strict_json_schema=True,\n",
      "              is_enabled=True,\n",
      "              tool_input_guardrails=None,\n",
      "              tool_output_guardrails=None)]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(upgraded_research_paper_assistant.tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b54a82ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_result_with_tools = await Runner.run(\n",
    "    starting_agent=upgraded_research_paper_assistant,\n",
    "    input=(\n",
    "        \"Get me information on rover navigation, planetary data collection, mission planning, resource allocation, or other related fields \"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9b61f988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ModelResponse(output=[ResponseFunctionToolCall(arguments='{\\n        \"user_query\": \"rover navigation\",\\n        \"retrieval_mode\": \"hybrid\",\\n        \"top_k\": 5\\n      }', call_id='call_HVbzaNXsZ3k5Q25uPEcAKXnR', name='get_research_papers', type='function_call', id='fc_09e72c69354cca1500690e53e8fd1c8197b2c1577418c07dd3', status='completed'), ResponseFunctionToolCall(arguments='{\\n        \"user_query\": \"planetary data collection\",\\n        \"retrieval_mode\": \"hybrid\",\\n        \"top_k\": 5\\n      }', call_id='call_u3zPtdh25hYg8Vca1BlS8y5I', name='get_research_papers', type='function_call', id='fc_09e72c69354cca1500690e53ea7d54819780982729e0c4bbb5', status='completed'), ResponseFunctionToolCall(arguments='{\\n        \"user_query\": \"mission planning\",\\n        \"retrieval_mode\": \"hybrid\",\\n        \"top_k\": 5\\n      }', call_id='call_atL4jegMmNC2cJDtFR2RP4Xn', name='get_research_papers', type='function_call', id='fc_09e72c69354cca1500690e53eca7548197b111f35991fd0216', status='completed'), ResponseFunctionToolCall(arguments='{\\n        \"user_query\": \"resource allocation in space missions\",\\n        \"retrieval_mode\": \"hybrid\",\\n        \"top_k\": 5\\n      }', call_id='call_tK1uRHgxS62YlR5mx9kXJw76', name='get_research_papers', type='function_call', id='fc_09e72c69354cca1500690e53ee2f4481979b95f8010eb3b8ae', status='completed')], usage=Usage(requests=1, input_tokens=85, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=232, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=317, request_usage_entries=[]), response_id='resp_09e72c69354cca1500690e53e5e8948197a2704a53fd8490e3'), ModelResponse(output=[ResponseOutputMessage(id='msg_09e72c69354cca1500690e53f43f9c819796bcaf956f336f17', content=[ResponseOutputText(annotations=[], text=\"Unfortunately, no research papers were found related to 'rover navigation', 'planetary data collection', and 'resource allocation in space missions'. However, I found a paper related to 'mission planning':\\n\\n1) The nucleus of 103P/Hartley 2, target of the EPOXI mission\\nAbstract: 103P/Hartley 2 was selected as the target comet for the Deep Impact extended mission, EPOXI, in October 2007. There have been no direct optical observations of the nucleus of this comet, as it has always been highly active when previously observed. This paper discusses its observations to confirm that it had not broken up and was in the predicted position, to provide astrometry and brightness information for mission planning, and to continue the characterisation of the nucleus. The study suggests that the comet had no visible coma, although there was faint activity possibly due to the dust trail from previous activity. The paper indicates that the nucleus radius of the comet is less than or equal to 1 km.\\n\\nI'll continue to look for more relevant research in these domains.\", type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], usage=Usage(requests=1, input_tokens=984, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=223, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=1207, request_usage_entries=[]), response_id='resp_09e72c69354cca1500690e53f2befc8197ad7c35e1c8b30bce')]\n"
     ]
    }
   ],
   "source": [
    "print(run_result_with_tools.raw_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a57b24",
   "metadata": {},
   "source": [
    "## Agent as Tools (Ochestration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ef7841",
   "metadata": {},
   "source": [
    "Add an image of the flow of these agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1a3dafca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define specialized agents for different research retrieval tasks\n",
    "research_paper_agent = Agent(\n",
    "    name=\"research_paper_agent\",\n",
    "    instructions=\"\"\"\n",
    "        You specialize in retrieving and summarizing academic research papers.\n",
    "        Use the get_research_papers tool to find relevant literature based on the user's query.\n",
    "        Always cite sources using [1], [2], etc., and focus on summarizing key findings,\n",
    "        methodologies, and implications of the studies retrieved.\n",
    "    \"\"\",\n",
    "    handoff_description=\"A research retrieval specialist with access to academic papers and literature databases.\",\n",
    "    tools=[get_research_papers],\n",
    ")\n",
    "\n",
    "research_conversation_agent = Agent(\n",
    "    name=\"research_conversation_agent\",\n",
    "    instructions=\"\"\"\n",
    "        You specialize in retrieving and summarizing past research discussions and analyses.\n",
    "        Use the get_past_research_conversations tool to surface relevant prior sessions\n",
    "        or summaries that relate to the user's current topic of inquiry.\n",
    "        Present these as context and examples of prior analytical reasoning.\n",
    "    \"\"\",\n",
    "    handoff_description=\"A research memory specialist with access to prior academic discussions and analyses.\",\n",
    "    tools=[get_past_research_conversations],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "304cac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an orchestrator agent that can coordinate both research retrieval agents\n",
    "orchestrator_agent = Agent(\n",
    "    name=\"research_assistant_orchestrator\",\n",
    "    instructions=(\n",
    "        \"You are a Research Orchestrator Assistant responsible for coordinating information retrieval \"\n",
    "        \"across multiple specialized research tools.\\n\\n\"\n",
    "        \"Your role is to help users explore, analyze, and synthesize academic research efficiently.\\n\\n\"\n",
    "        \"IMPORTANT RULES:\\n\"\n",
    "        \"1. ALWAYS use translate_to_research_papers when a query mentions research papers, studies, or findings.\\n\"\n",
    "        \"2. ALWAYS use translate_to_research_conversations when a query mentions previous discussions, analyses, or summaries.\\n\"\n",
    "        \"3. If a query requests BOTH new research and past discussions, use BOTH tools in sequence.\\n\"\n",
    "        \"4. NEVER attempt to provide research summaries without using your tools.\\n\"\n",
    "        \"5. Each tool provides complementary context ‚Äî use all appropriate tools for a comprehensive academic response.\\n\\n\"\n",
    "        \"After retrieving relevant results, synthesize them into a cohesive summary:\\n\"\n",
    "        \"- Clearly distinguish between newly retrieved research and recalled past discussions.\\n\"\n",
    "        \"- Cite sources using [1], [2], etc.\\n\"\n",
    "        \"- Identify key insights, trends, and research gaps.\\n\"\n",
    "        \"- Maintain an academic and objective tone.\"\n",
    "    ),\n",
    "    tools=[\n",
    "        research_paper_agent.as_tool(\n",
    "            tool_name=\"translate_to_research_papers\",\n",
    "            tool_description=\"Retrieve and summarize relevant academic research papers and literature findings.\",\n",
    "        ),\n",
    "        research_conversation_agent.as_tool(\n",
    "            tool_name=\"translate_to_research_conversations\",\n",
    "            tool_description=\"Retrieve and summarize past research discussions or analyses related to the topic.\",\n",
    "        ),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "62aab9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final agent to synthesize information from all sources (Research use case)\n",
    "synthesizer_agent = Agent(\n",
    "    name=\"research_response_synthesizer\",\n",
    "    instructions=(\n",
    "        \"You create comprehensive, well-organized research summaries by combining information from multiple sources.\\n\\n\"\n",
    "        \"When organizing your response:\\n\"\n",
    "        \"1) Start with a concise abstract-style overview (3‚Äì5 sentences) highlighting key findings and takeaways.\\n\"\n",
    "        \"2) Clearly separate NEW LITERATURE FINDINGS from PAST RESEARCH DISCUSSIONS.\\n\"\n",
    "        \"3) Cite sources using bracketed numbers [1], [2], etc., aligned with the retrieved items.\\n\"\n",
    "        \"4) Emphasize methods, evidence strength, and limitations; avoid speculation beyond the provided context.\\n\"\n",
    "        \"5) Use clear, scannable formatting (short paragraphs, bullet points where appropriate).\\n\"\n",
    "        \"6) Conclude with open questions, gaps, or future work suggested by the literature.\\n\"\n",
    "        \"7) If evidence is sparse, state this explicitly and avoid overgeneralization.\\n\"\n",
    "        \"Tone: academic, objective, and precise.\"\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9f02800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import ItemHelpers, MessageOutputItem, trace\n",
    "from agents import Runner  # assuming Runner is imported elsewhere; include here for clarity\n",
    "\n",
    "\n",
    "async def research_assistant_workflow(user_query: str):\n",
    "    \"\"\"Run the complete research assistant workflow (orchestrate retrieval + synthesize).\"\"\"\n",
    "    # 1) Have the research orchestrator decide which tools to invoke\n",
    "    with trace(\"Research Orchestrator\"):\n",
    "        orchestrator_result = await Runner.run(orchestrator_agent, user_query)\n",
    "\n",
    "        # Debug/transparency: print intermediate orchestration steps\n",
    "        print(\"\\n--- Research Orchestration Steps ---\")\n",
    "        for item in orchestrator_result.new_items:\n",
    "            if isinstance(item, MessageOutputItem):\n",
    "                text = ItemHelpers.text_message_output(item)\n",
    "                if text:\n",
    "                    print(f\"  - Retrieval step: {text}\")\n",
    "\n",
    "        # 2) Synthesize all gathered information into a cohesive research summary\n",
    "        synthesizer_result = await Runner.run(\n",
    "            synthesizer_agent, orchestrator_result.to_input_list()\n",
    "        )\n",
    "\n",
    "        print(f\"\\n\\n--- Final Research Synthesis ---\\n{synthesizer_result.final_output}\\n\")\n",
    "\n",
    "    return synthesizer_result.final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cc193e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio to patch the event loop\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8bc86489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_virtual_research_assistant(query):\n",
    "    # Create a new event loop\n",
    "    loop = asyncio.new_event_loop()\n",
    "    asyncio.set_event_loop(loop)\n",
    "\n",
    "    # Run the async function and get the result\n",
    "    result = loop.run_until_complete(research_assistant_workflow(query))\n",
    "\n",
    "    # Clean up\n",
    "    loop.close()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b705b630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Research Orchestration Steps ---\n",
      "  - Retrieval step: Here is a recent research paper relevant to the topic of mission planning:\n",
      "\n",
      "Recent Research Findings:\n",
      "- A significant paper discusses mission planning in the context of planetary science, specifically the EPOXI mission targeting comet 103P/Hartley 2. The research details how mission planning required direct observations to confirm the comet‚Äôs nucleus was intact and correctly positioned‚Äîcritical information for mission trajectory and rendezvous planning. The team used photometric and astrometric observations to support these efforts and to characterize the nucleus' size and activity level. Their analysis of faint cometary activity provided essential parameters (nucleus size, albedo, and ongoing activity) informing safe and accurate mission trajectory planning [1].\n",
      "\n",
      "Key Insight:\n",
      "- This work highlights the importance of celestial body characterization and up-to-date astrometric data as foundational elements in successful space mission planning.\n",
      "\n",
      "If you are interested in mission planning research from other domains (like robotics, terrestrial military operations, AI, or business applications), please specify, and I can retrieve domain-specific papers.\n",
      "\n",
      "---\n",
      "[1] Snodgrass et al., \"The nucleus of 103P/Hartley 2, target of the EPOXI mission\"\n",
      "\n",
      "\n",
      "--- Final Research Synthesis ---\n",
      "**Abstract Overview**  \n",
      "Recent literature on mission planning highlights its critical role in successful space missions, combining real-time observation, object characterization, and trajectory calculations. A prominent case study‚Äîfocused on the EPOXI mission to comet 103P/Hartley 2‚Äîdemonstrates the integration of astrometric confirmation and photometric measurements for safe spacecraft navigation and accurate target rendezvous. This research underscores the necessity of direct observational data in refining mission parameters and ensuring the viability of celestial body encounters [1].\n",
      "\n",
      "---\n",
      "\n",
      "**NEW LITERATURE FINDINGS**\n",
      "\n",
      "- **EPOXI Mission Planning Case Study**  \n",
      "  - The EPOXI mission‚Äôs planning phase relied on targeted photometric and astrometric observations at multiple epochs to ensure the comet‚Äôs nucleus was both intact and precisely located [1].\n",
      "  - Detections of faint activity near the nucleus were attributed to ongoing outgassing from the previous perihelion, rather than new activity, informing the timeline and approach strategy.\n",
      "  - Observations yielded a nucleus radius estimate ‚â§1 km (assuming 4% albedo, with some evidence suggesting 6%), a vital constraint for spacecraft trajectory and close approach [1].\n",
      "  - These data served as real-time updates for mission designers, enabling trajectory refinements and risk mitigation.\n",
      "\n",
      "- **Research Methods**\n",
      "  - Ground-based telescopes provided repeated photometric and astrometric data.\n",
      "  - Nucleus activity assessments used brightness and spatial distribution patterns.\n",
      "\n",
      "- **Evidence Strength & Limitations**\n",
      "  - Direct observation offered strong real-time evidence for planning.\n",
      "  - Limitations included inability to rule out multiple fragments and assumptions about albedo affecting size estimation.\n",
      "\n",
      "---\n",
      "\n",
      "**PAST RESEARCH DISCUSSION**\n",
      "\n",
      "- Previous planetary mission planning often relied on historical observations, which sometimes led to targeting objects later found to be lost, fragmented, or otherwise unreachable.\n",
      "- Advances in observation and data assimilation have shifted best practices toward continual, pre-encounter monitoring and rapid parameter adjustment.\n",
      "\n",
      "---\n",
      "\n",
      "**Conclusion: Open Questions & Future Directions**\n",
      "\n",
      "- There is a need for even more rapid and automated methods of updating mission planning parameters as new observational data become available.\n",
      "- Uncertainties remain about accurately estimating target body properties (e.g., precise size, albedo, fragmentation state) especially for small, distant targets.\n",
      "- How to generalize these methods to other domains (autonomous robotics, Earth observation, military planning) remains an active area for research.\n",
      "\n",
      "**Evidence Gaps**\n",
      "- Current evidence is primarily from case studies in planetary science; comprehensive, cross-domain reviews are sparse.\n",
      "- Further research is warranted on automated, adaptive mission planning frameworks.\n",
      "\n",
      "---\n",
      "\n",
      "**Reference**  \n",
      "[1] Snodgrass et al., \"The nucleus of 103P/Hartley 2, target of the EPOXI mission\"\n",
      "  \n",
      "*If you require research on mission planning in other domains (e.g., robotics, military, business), please specify for more targeted literature.*\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'**Abstract Overview**  \\nRecent literature on mission planning highlights its critical role in successful space missions, combining real-time observation, object characterization, and trajectory calculations. A prominent case study‚Äîfocused on the EPOXI mission to comet 103P/Hartley 2‚Äîdemonstrates the integration of astrometric confirmation and photometric measurements for safe spacecraft navigation and accurate target rendezvous. This research underscores the necessity of direct observational data in refining mission parameters and ensuring the viability of celestial body encounters [1].\\n\\n---\\n\\n**NEW LITERATURE FINDINGS**\\n\\n- **EPOXI Mission Planning Case Study**  \\n  - The EPOXI mission‚Äôs planning phase relied on targeted photometric and astrometric observations at multiple epochs to ensure the comet‚Äôs nucleus was both intact and precisely located [1].\\n  - Detections of faint activity near the nucleus were attributed to ongoing outgassing from the previous perihelion, rather than new activity, informing the timeline and approach strategy.\\n  - Observations yielded a nucleus radius estimate ‚â§1 km (assuming 4% albedo, with some evidence suggesting 6%), a vital constraint for spacecraft trajectory and close approach [1].\\n  - These data served as real-time updates for mission designers, enabling trajectory refinements and risk mitigation.\\n\\n- **Research Methods**\\n  - Ground-based telescopes provided repeated photometric and astrometric data.\\n  - Nucleus activity assessments used brightness and spatial distribution patterns.\\n\\n- **Evidence Strength & Limitations**\\n  - Direct observation offered strong real-time evidence for planning.\\n  - Limitations included inability to rule out multiple fragments and assumptions about albedo affecting size estimation.\\n\\n---\\n\\n**PAST RESEARCH DISCUSSION**\\n\\n- Previous planetary mission planning often relied on historical observations, which sometimes led to targeting objects later found to be lost, fragmented, or otherwise unreachable.\\n- Advances in observation and data assimilation have shifted best practices toward continual, pre-encounter monitoring and rapid parameter adjustment.\\n\\n---\\n\\n**Conclusion: Open Questions & Future Directions**\\n\\n- There is a need for even more rapid and automated methods of updating mission planning parameters as new observational data become available.\\n- Uncertainties remain about accurately estimating target body properties (e.g., precise size, albedo, fragmentation state) especially for small, distant targets.\\n- How to generalize these methods to other domains (autonomous robotics, Earth observation, military planning) remains an active area for research.\\n\\n**Evidence Gaps**\\n- Current evidence is primarily from case studies in planetary science; comprehensive, cross-domain reviews are sparse.\\n- Further research is warranted on automated, adaptive mission planning frameworks.\\n\\n---\\n\\n**Reference**  \\n[1] Snodgrass et al., \"The nucleus of 103P/Hartley 2, target of the EPOXI mission\"\\n  \\n*If you require research on mission planning in other domains (e.g., robotics, military, business), please specify for more targeted literature.*'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now call the function this way\n",
    "query = input(\"What research topic can I help you with today? \")\n",
    "run_virtual_research_assistant(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67e6c26",
   "metadata": {},
   "source": [
    "## Agentic Chat System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "05741d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Table chat_history created successfully with index.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import uuid\n",
    "\n",
    "# Create chat_history table in Oracle\n",
    "with conn.cursor() as cur:\n",
    "    # Drop table if exists (for development)\n",
    "    cur.execute(\"\"\"\n",
    "        BEGIN\n",
    "            EXECUTE IMMEDIATE 'DROP TABLE chat_history';\n",
    "        EXCEPTION WHEN OTHERS THEN\n",
    "            IF SQLCODE != -942 THEN RAISE; END IF;\n",
    "        END;\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create chat_history table\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE chat_history (\n",
    "            id VARCHAR2(100) PRIMARY KEY,\n",
    "            thread_id VARCHAR2(100) NOT NULL,\n",
    "            role VARCHAR2(20) NOT NULL,\n",
    "            message CLOB NOT NULL,\n",
    "            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "        TABLESPACE USERS\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create index on thread_id and timestamp for efficient retrieval\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE INDEX idx_thread_timestamp \n",
    "        ON chat_history(thread_id, timestamp)\n",
    "        TABLESPACE USERS\n",
    "    \"\"\")\n",
    "    \n",
    "    conn.commit()\n",
    "    print(\"‚úÖ Table chat_history created successfully with index.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "34c470ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def research_assistant_chat(user_query, thread_id=None):\n",
    "    \"\"\"\n",
    "    Run the complete research assistant workflow with conversation history.\n",
    "    For each conversation turn:\n",
    "      - Stores the user's input and the assistant's output in Oracle along with a timestamp and thread_id.\n",
    "      - Retrieves and appends previous conversation history (ordered by timestamp) to the agent's input.\n",
    "    \n",
    "    If no thread_id is provided, a new conversation session is started.\n",
    "    \n",
    "    Returns:\n",
    "      tuple: (final_output, thread_id) where thread_id is the session identifier.\n",
    "    \"\"\"\n",
    "    # Generate a new thread id if not provided\n",
    "    if thread_id is None:\n",
    "        thread_id = str(uuid.uuid4())\n",
    "        print(f\"üìù New research conversation started with thread ID: {thread_id}\")\n",
    "    else:\n",
    "        print(f\"üìù Continuing research conversation with thread ID: {thread_id}\")\n",
    "    \n",
    "    # --- Step 1: Store the new user query in Oracle ---\n",
    "    message_id = str(uuid.uuid4())\n",
    "    \n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO chat_history (id, thread_id, role, message, timestamp)\n",
    "            VALUES (:id, :thread_id, :role, :message, CURRENT_TIMESTAMP)\n",
    "        \"\"\", {\n",
    "            'id': message_id,\n",
    "            'thread_id': thread_id,\n",
    "            'role': 'user',\n",
    "            'message': user_query\n",
    "        })\n",
    "        conn.commit()\n",
    "    \n",
    "    # --- Step 2: Retrieve full conversation history for context ---\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT role, message, timestamp\n",
    "            FROM chat_history\n",
    "            WHERE thread_id = :thread_id\n",
    "            ORDER BY timestamp ASC\n",
    "        \"\"\", {'thread_id': thread_id})\n",
    "        \n",
    "        chat_history = cur.fetchall()\n",
    "    \n",
    "    conversation_context = \"\"\n",
    "    for entry in chat_history:\n",
    "        role, message, timestamp = entry\n",
    "        if role == \"user\":\n",
    "            conversation_context += f\"User: {message}\\n\"\n",
    "        else:\n",
    "            conversation_context += f\"Assistant: {message}\\n\"\n",
    "    \n",
    "    # --- Step 3: Run the orchestrator agent with the conversation context ---\n",
    "    with trace(\"Research Orchestrator\"):\n",
    "        orchestrator_result = await Runner.run(orchestrator_agent, conversation_context)\n",
    "    \n",
    "    # Print intermediate processing steps for debugging/transparency\n",
    "    print(\"\\n--- Research Orchestrator Processing Steps ---\")\n",
    "    for item in orchestrator_result.new_items:\n",
    "        if isinstance(item, MessageOutputItem):\n",
    "            text = ItemHelpers.text_message_output(item)\n",
    "            if text:\n",
    "                print(f\"  - Information gathering step: {text}\")\n",
    "    \n",
    "    # --- Step 4: Run the synthesizer agent to produce a cohesive response ---\n",
    "    synthesizer_result = await Runner.run(\n",
    "        synthesizer_agent, orchestrator_result.to_input_list()\n",
    "    )\n",
    "    \n",
    "    # --- Step 5: Store the assistant's final output in Oracle ---\n",
    "    response_id = str(uuid.uuid4())\n",
    "    \n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO chat_history (id, thread_id, role, message, timestamp)\n",
    "            VALUES (:id, :thread_id, :role, :message, CURRENT_TIMESTAMP)\n",
    "        \"\"\", {\n",
    "            'id': response_id,\n",
    "            'thread_id': thread_id,\n",
    "            'role': 'assistant',\n",
    "            'message': synthesizer_result.final_output\n",
    "        })\n",
    "        conn.commit()\n",
    "    \n",
    "    print(f\"\\n\\n--- Final Research Response ---\\n{synthesizer_result.final_output}\\n\")\n",
    "    \n",
    "    return synthesizer_result.final_output, thread_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b6250b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_research_assistant_chat(query, thread_id=None):\n",
    "    \"\"\"\n",
    "    Run the research assistant synchronously.\n",
    "    Optionally, a thread_id can be provided to continue an existing conversation.\n",
    "    Returns a tuple (final_output, thread_id).\n",
    "    \"\"\"\n",
    "    # Create a new event loop\n",
    "    loop = asyncio.new_event_loop()\n",
    "    asyncio.set_event_loop(loop)\n",
    "    \n",
    "    # Run the async function and get the result\n",
    "    result, thread_id = loop.run_until_complete(\n",
    "        research_assistant_chat(query, thread_id=thread_id)\n",
    "    )\n",
    "    \n",
    "    # Clean up the loop\n",
    "    loop.close()\n",
    "    \n",
    "    return result, thread_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e7cc560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_chat_session():\n",
    "    \"\"\"\n",
    "    Launches a research chat session that continues until the user enters 'q', 'exit', or 'quit'.\n",
    "    The session uses a persistent thread_id to preserve conversation history.\n",
    "    \"\"\"\n",
    "    print(\"üî¨ Starting Research Paper Assistant Chat\")\n",
    "    print(\"Type 'q', 'exit' or 'quit' to exit.\\n\")\n",
    "    \n",
    "    session_thread_id = None\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"What research topic can I help you with today? \")\n",
    "        \n",
    "        if query.lower() in [\"q\", \"exit\", \"quit\"]:\n",
    "            print(\"Exiting research chat session.\")\n",
    "            break\n",
    "        \n",
    "        response, session_thread_id = run_research_assistant_chat(\n",
    "            query, thread_id=session_thread_id\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüìö Assistant: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cf336fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Starting Research Paper Assistant Chat\n",
      "Type 'q', 'exit' or 'quit' to exit.\n",
      "\n",
      "üìù New research conversation started with thread ID: 12c5139f-3c69-4ce5-ad3f-e948c0edf083\n",
      "\n",
      "--- Research Orchestrator Processing Steps ---\n",
      "  - Information gathering step: Here is a relevant academic research paper on mission planning:\n",
      "\n",
      "1. The nucleus of 103P/Hartley 2, target of the EPOXI mission  \n",
      "This paper details the mission planning process for the EPOXI mission, focusing on its approach to studying the comet 103P/Hartley 2. It emphasizes the importance of pre-mission observational data‚Äîsuch as astrometry and photometry‚Äîto confirm the target‚Äôs status, refine trajectory planning, and ensure the success of scientific instruments. The study illustrates how rigorous pre-mission assessments are vital for reducing mission risks and maximizing scientific return, especially when dealing with dynamic or poorly observed targets [1].\n",
      "\n",
      "If you require research on mission planning applied to a specific field (e.g., robotics, military, unmanned vehicles, etc.), please let me know for more specialized sources.\n",
      "\n",
      "---\n",
      "[1] \"The nucleus of 103P/Hartley 2, target of the EPOXI mission\"\n",
      "\n",
      "\n",
      "--- Final Research Response ---\n",
      "ABSTRACT  \n",
      "Mission planning is a broad field encompassing techniques and technologies for organizing, scheduling, and optimizing a mission‚Äôs operational timeline to accomplish defined objectives. In the context of spacecraft missions, recent literature highlights advances in computational tools and pre-mission data collection for risk reduction and science optimization. Careful pre-mission observation and planning remain central to mission success, particularly when the targets involve dynamic or poorly characterized environments.\n",
      "\n",
      "NEW LITERATURE FINDINGS  \n",
      "A key recent publication focuses on planning for the EPOXI mission to comet 103P/Hartley 2 [1].  \n",
      "- The research emphasizes pre-mission astrometric and photometric observations to confirm the comet‚Äôs exact position and state (ensuring it was intact and detectable).  \n",
      "- Methodologies included multi-epoch astrometry and brightness data collection to assess the size, albedo, and activity level of the comet‚Äôs nucleus.  \n",
      "- Findings directly supported mission trajectory planning and calibration of onboard instruments, demonstrating the operational value of early, mission-specific, scientific observations.\n",
      "\n",
      "PAST RESEARCH DISCUSSIONS  \n",
      "- Tradition holds that mission planning frequently leverages direct observational data, leveraging astrometry, remote sensing, and simulation to inform route planning, instrument readiness, and contingency frameworks.\n",
      "- Historical missions, such as previous comet flybys and planetary landers, outline the importance of timely data acquisition and flexibility in planning to address evolving mission constraints and uncertainties.\n",
      "\n",
      "METHODS, EVIDENCE STRENGTH & LIMITATIONS  \n",
      "- The cited study uses robust, direct observational data (astrometry, multi-epoch photometry), which strengthens confidence in pre-launch characterization.\n",
      "- Specific focus on a single target (103P/Hartley 2) limits the generalizability of operational lessons to all mission types, but the procedure is applicable broadly.\n",
      "- No direct comparative analysis with alternate planning techniques was performed.\n",
      "\n",
      "CONCLUSIONS & OPEN QUESTIONS  \n",
      "- Comprehensive, target-specific observation prior to mission execution remains a best practice.\n",
      "- Future work is needed to validate these approaches across different mission contexts (e.g., interplanetary versus terrestrial robotics).\n",
      "- Key gaps include comparative assessments of automated versus manual planning methods, and observation-to-planning feedback loops.\n",
      "\n",
      "EVIDENCE GAP  \n",
      "- Currently, literature specific to mission planning remains focused on single-case studies or domain-specific solutions. Broader, cross-domain investigations and comparisons of methodologies are limited.\n",
      "\n",
      "---\n",
      "[1] \"The nucleus of 103P/Hartley 2, target of the EPOXI mission\".  \n",
      "If you would like research specific to another domain (e.g. robotics, military, autonomous vehicles), please specify for a more tailored review.\n",
      "\n",
      "\n",
      "üìö Assistant: ABSTRACT  \n",
      "Mission planning is a broad field encompassing techniques and technologies for organizing, scheduling, and optimizing a mission‚Äôs operational timeline to accomplish defined objectives. In the context of spacecraft missions, recent literature highlights advances in computational tools and pre-mission data collection for risk reduction and science optimization. Careful pre-mission observation and planning remain central to mission success, particularly when the targets involve dynamic or poorly characterized environments.\n",
      "\n",
      "NEW LITERATURE FINDINGS  \n",
      "A key recent publication focuses on planning for the EPOXI mission to comet 103P/Hartley 2 [1].  \n",
      "- The research emphasizes pre-mission astrometric and photometric observations to confirm the comet‚Äôs exact position and state (ensuring it was intact and detectable).  \n",
      "- Methodologies included multi-epoch astrometry and brightness data collection to assess the size, albedo, and activity level of the comet‚Äôs nucleus.  \n",
      "- Findings directly supported mission trajectory planning and calibration of onboard instruments, demonstrating the operational value of early, mission-specific, scientific observations.\n",
      "\n",
      "PAST RESEARCH DISCUSSIONS  \n",
      "- Tradition holds that mission planning frequently leverages direct observational data, leveraging astrometry, remote sensing, and simulation to inform route planning, instrument readiness, and contingency frameworks.\n",
      "- Historical missions, such as previous comet flybys and planetary landers, outline the importance of timely data acquisition and flexibility in planning to address evolving mission constraints and uncertainties.\n",
      "\n",
      "METHODS, EVIDENCE STRENGTH & LIMITATIONS  \n",
      "- The cited study uses robust, direct observational data (astrometry, multi-epoch photometry), which strengthens confidence in pre-launch characterization.\n",
      "- Specific focus on a single target (103P/Hartley 2) limits the generalizability of operational lessons to all mission types, but the procedure is applicable broadly.\n",
      "- No direct comparative analysis with alternate planning techniques was performed.\n",
      "\n",
      "CONCLUSIONS & OPEN QUESTIONS  \n",
      "- Comprehensive, target-specific observation prior to mission execution remains a best practice.\n",
      "- Future work is needed to validate these approaches across different mission contexts (e.g., interplanetary versus terrestrial robotics).\n",
      "- Key gaps include comparative assessments of automated versus manual planning methods, and observation-to-planning feedback loops.\n",
      "\n",
      "EVIDENCE GAP  \n",
      "- Currently, literature specific to mission planning remains focused on single-case studies or domain-specific solutions. Broader, cross-domain investigations and comparisons of methodologies are limited.\n",
      "\n",
      "---\n",
      "[1] \"The nucleus of 103P/Hartley 2, target of the EPOXI mission\".  \n",
      "If you would like research specific to another domain (e.g. robotics, military, autonomous vehicles), please specify for a more tailored review.\n",
      "\n",
      "üìù Continuing research conversation with thread ID: 12c5139f-3c69-4ce5-ad3f-e948c0edf083\n",
      "\n",
      "--- Research Orchestrator Processing Steps ---\n",
      "  - Information gathering step: Your first query was:\n",
      "\n",
      "\"hi get me papers on mission planning\"\n",
      "\n",
      "This was a request for academic research papers related to the topic of mission planning.\n",
      "\n",
      "\n",
      "--- Final Research Response ---\n",
      "Your first query was:  \n",
      "\"hi get me papers on mission planning\"\n",
      "\n",
      "You requested research literature relevant to the topic of mission planning.\n",
      "\n",
      "\n",
      "üìö Assistant: Your first query was:  \n",
      "\"hi get me papers on mission planning\"\n",
      "\n",
      "You requested research literature relevant to the topic of mission planning.\n",
      "\n",
      "Exiting research chat session.\n"
     ]
    }
   ],
   "source": [
    "# Start the research chat session\n",
    "research_chat_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5af5bfe",
   "metadata": {},
   "source": [
    "## Session Memory with Oracle AI Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e20956c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Union\n",
    "from datetime import datetime\n",
    "import oracledb\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "class OracleSession:\n",
    "    \"\"\"Custom Oracle session implementation following the Session protocol\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        session_id: str, \n",
    "        connection,\n",
    "        table_name: str = \"chat_history\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize Oracle session storage.\n",
    "        \n",
    "        Args:\n",
    "            session_id: Unique identifier for this conversation session\n",
    "            connection: Active oracledb connection object\n",
    "            table_name: Name of the Oracle table storing session data\n",
    "        \"\"\"\n",
    "        self.session_id = session_id\n",
    "        self.conn = connection\n",
    "        self.table_name = table_name\n",
    "    \n",
    "    async def get_items(self, limit: Optional[int] = None) -> List[dict]:\n",
    "        \"\"\"Retrieve conversation history for this session\"\"\"\n",
    "        try:\n",
    "            with self.conn.cursor() as cur:\n",
    "                if limit:\n",
    "                    cur.execute(f\"\"\"\n",
    "                        SELECT message\n",
    "                        FROM {self.table_name}\n",
    "                        WHERE thread_id = :session_id\n",
    "                        ORDER BY timestamp ASC\n",
    "                        FETCH FIRST :limit ROWS ONLY\n",
    "                    \"\"\", {'session_id': self.session_id, 'limit': limit})\n",
    "                else:\n",
    "                    cur.execute(f\"\"\"\n",
    "                        SELECT message\n",
    "                        FROM {self.table_name}\n",
    "                        WHERE thread_id = :session_id\n",
    "                        ORDER BY timestamp ASC\n",
    "                    \"\"\", {'session_id': self.session_id})\n",
    "                \n",
    "                rows = cur.fetchall()\n",
    "                \n",
    "                items = []\n",
    "                for row in rows:\n",
    "                    # Deserialize JSON from CLOB\n",
    "                    message_clob = row[0]\n",
    "                    if message_clob:\n",
    "                        message_str = message_clob.read() if hasattr(message_clob, 'read') else str(message_clob)\n",
    "                        items.append(json.loads(message_str))\n",
    "                \n",
    "                return items\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving items: {e}\")\n",
    "            return []\n",
    "    \n",
    "    async def add_items(self, items: List[dict]) -> None:\n",
    "        \"\"\"Store new items for this session\"\"\"\n",
    "        try:\n",
    "            with self.conn.cursor() as cur:\n",
    "                for item in items:\n",
    "                    item_id = str(uuid.uuid4())\n",
    "                    \n",
    "                    # Serialize the entire item as JSON\n",
    "                    message_json = json.dumps(item)\n",
    "                    \n",
    "                    # Extract role if available, otherwise default to 'system'\n",
    "                    role = item.get('role', 'system')\n",
    "                    \n",
    "                    cur.execute(f\"\"\"\n",
    "                        INSERT INTO {self.table_name} (id, thread_id, role, message, timestamp)\n",
    "                        VALUES (:id, :session_id, :role, :message, CURRENT_TIMESTAMP)\n",
    "                    \"\"\", {\n",
    "                        'id': item_id,\n",
    "                        'session_id': self.session_id,\n",
    "                        'role': role,\n",
    "                        'message': message_json\n",
    "                    })\n",
    "                \n",
    "                self.conn.commit()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding items: {e}\")\n",
    "            self.conn.rollback()\n",
    "    \n",
    "    async def pop_item(self, limit: Optional[int] = None) -> Optional[Union[dict, List[dict]]]:\n",
    "        \"\"\"\n",
    "        Remove and return the most recent item(s) for this session.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with self.conn.cursor() as cur:\n",
    "                # Pop a single most-recent item\n",
    "                if not limit or limit <= 1:\n",
    "                    cur.execute(f\"\"\"\n",
    "                        SELECT id, message\n",
    "                        FROM {self.table_name}\n",
    "                        WHERE thread_id = :session_id\n",
    "                        ORDER BY timestamp DESC\n",
    "                        FETCH FIRST 1 ROW ONLY\n",
    "                    \"\"\", {'session_id': self.session_id})\n",
    "                    \n",
    "                    row = cur.fetchone()\n",
    "                    \n",
    "                    if row:\n",
    "                        item_id, message_clob = row\n",
    "                        message_str = message_clob.read() if hasattr(message_clob, 'read') else str(message_clob)\n",
    "                        item = json.loads(message_str)\n",
    "                        \n",
    "                        # Delete the item\n",
    "                        cur.execute(f\"\"\"\n",
    "                            DELETE FROM {self.table_name}\n",
    "                            WHERE id = :id\n",
    "                        \"\"\", {'id': item_id})\n",
    "                        \n",
    "                        self.conn.commit()\n",
    "                        return item\n",
    "                    \n",
    "                    return None\n",
    "                \n",
    "                # Pop multiple most-recent items\n",
    "                cur.execute(f\"\"\"\n",
    "                    SELECT id, message\n",
    "                    FROM {self.table_name}\n",
    "                    WHERE thread_id = :session_id\n",
    "                    ORDER BY timestamp DESC\n",
    "                    FETCH FIRST :limit ROWS ONLY\n",
    "                \"\"\", {'session_id': self.session_id, 'limit': limit})\n",
    "                \n",
    "                rows = cur.fetchall()\n",
    "                \n",
    "                if not rows:\n",
    "                    return []\n",
    "                \n",
    "                items = []\n",
    "                ids_to_delete = []\n",
    "                \n",
    "                for row in rows:\n",
    "                    item_id, message_clob = row\n",
    "                    message_str = message_clob.read() if hasattr(message_clob, 'read') else str(message_clob)\n",
    "                    items.append(json.loads(message_str))\n",
    "                    ids_to_delete.append(item_id)\n",
    "                \n",
    "                # Delete all items\n",
    "                for item_id in ids_to_delete:\n",
    "                    cur.execute(f\"\"\"\n",
    "                        DELETE FROM {self.table_name}\n",
    "                        WHERE id = :id\n",
    "                    \"\"\", {'id': item_id})\n",
    "                \n",
    "                self.conn.commit()\n",
    "                return items\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error popping item(s): {e}\")\n",
    "            self.conn.rollback()\n",
    "            return None if (not limit or limit <= 1) else []\n",
    "    \n",
    "    async def clear_session(self) -> None:\n",
    "        \"\"\"Clear all items for this session\"\"\"\n",
    "        try:\n",
    "            with self.conn.cursor() as cur:\n",
    "                cur.execute(f\"\"\"\n",
    "                    DELETE FROM {self.table_name}\n",
    "                    WHERE thread_id = :session_id\n",
    "                \"\"\", {'session_id': self.session_id})\n",
    "                \n",
    "                self.conn.commit()\n",
    "                print(f\"‚úÖ Session {self.session_id} cleared successfully.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error clearing session: {e}\")\n",
    "            self.conn.rollback()\n",
    "    \n",
    "    def close(self) -> None:\n",
    "        \"\"\"\n",
    "        Note: Connection is managed externally, so we don't close it here.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079aa0f1",
   "metadata": {},
   "source": [
    "Basic Example of an Agent with Session Memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ef7a559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent\n",
    "research_agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=\"Research the topic and return the most relevant information.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fb1433a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Oracle session instance\n",
    "session = OracleSession(\n",
    "    session_id=\"conversation_123\", \n",
    "    connection=conn,\n",
    "    table_name=\"chat_history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f4cc72f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Hello Richmond! It‚Äôs great to meet someone working at the intersection of LLMs and agent memory‚Äîa rapidly evolving and impactful field.\n",
      "\n",
      "Here‚Äôs a concise exploration of **LLM memory architectures** and **agent memory systems**, with relevant concepts and recent research directions:\n",
      "\n",
      "---\n",
      "\n",
      "## **Large Language Models (LLMs) and Memory**\n",
      "\n",
      "**1. Intrinsic Memory Limitations:**\n",
      "- Standard large language models (e.g., GPT-4, PaLM, Llama) have a **context window**‚Äîthey can only \"remember\" a finite number of recent tokens (prompt length).\n",
      "- Information outside this window is inaccessible unless re-provided, limiting long-term coherence and continuity.\n",
      "\n",
      "\n",
      "**2. Memory Augmentation Approaches:**\n",
      "\n",
      "**a. External Memory Augmentation:**\n",
      "   - **Retrieval-Augmented Generation (RAG):** LLMs query external databases, vector stores, or document indices at runtime (e.g., KNN search on embeddings), allowing access to knowledge beyond the prompt window ([Lewis et al., 2020](https://arxiv.org/abs/2005.11401)).\n",
      "   - **Neural Turing Machines & Memory Networks:** Models with differentiable memory modules that can be read/written to support complex reasoning tasks.\n",
      "\n",
      "**b. Episodic Memory Architectures:**\n",
      "   - **Long-term Episodic Storage:** Logs past interactions or events (\"episodes\") and retrieves them contextually (e.g., via semantic search) to feed into the LLM for continuity ([Park et al., 2023 ‚Äì Generative Agents](https://arxiv.org/abs/2304.03442)).\n",
      "   - **Summarization Compression:** Long histories are summarized to create compact \"memories\" that are re-injected into the prompt (used in chatbots and open-world agents).\n",
      "\n",
      "**c. Recursive or Hierarchical Memory:**\n",
      "   - Organizes memory as nested summaries, e.g., day-wise or topic-wise aggregation, to facilitate scalable long-term reasoning.\n",
      "\n",
      "\n",
      "**3. Architectural Innovations:**\n",
      "- **Transformers with Extended Context Windows:** Models like MPT-8K, Claude, Gemini extend context windows to tens-of-thousands of tokens.\n",
      "- **Sparse Attention/Memory Layers:** Allow attending over much longer histories selectively (e.g., Longformer, Memory Transformer).\n",
      "\n",
      "---\n",
      "\n",
      "## **Agent Memory (Long-term Autonomous Agents)**\n",
      "\n",
      "- **Working Memory:** For immediate, short-term tasks (current plans, recent dialogue).\n",
      "- **Episodic Memory:** Stores experiential data (events, interactions) that can be retrieved later contextually.\n",
      "- **Semantic Memory:** Stores general world knowledge or facts (distinct from episodic).\n",
      "- **Procedural Memory:** Encodes skills, routines, or policy behaviors.\n",
      "\n",
      "**Key Challenges:**\n",
      "- **Relevance Filtering:** Selecting which memories to surface to the LLM at each step.\n",
      "- **Catastrophic Forgetting:** Avoiding loss of old memories as new ones are added.\n",
      "- **Efficient Indexing & Retrieval:** Fast, semantically meaningful lookup over large-scale memory banks.\n",
      "\n",
      "**Recent Developments:**\n",
      "- **Generative Agents (Stanford, 2023):** Agents with memory streams that simulate personalities, routines, and relationships.\n",
      "- **Forgetful LLMs/Selective Memory:** Techniques for intentional \"forgetting\" to manage agent memory size and relevance.\n",
      "\n",
      "---\n",
      "\n",
      "## **Resources and Further Reading**\n",
      "\n",
      "- **Papers:**\n",
      "  - [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (Lewis 2020)](https://arxiv.org/abs/2005.11401)\n",
      "  - [Generative Agents: Interactive Simulacra of Human Behavior (Park et al., 2023)](https://arxiv.org/abs/2304.03442)\n",
      "  - [Transformers with Memory](https://arxiv.org/abs/2006.11527) (Dai et al.)\n",
      "\n",
      "- **Frameworks:**\n",
      "  - [LangChain](https://langchain.com): Tools for constructing LLM-driven agents with external memory integrations.\n",
      "  - [LlamaIndex](https://www.llamaindex.ai/): Data framework for LLM memory and search.\n",
      "\n",
      "- **Notable Projects:**\n",
      "  - **AutoGPT, BabyAGI:** Open-source agents with persistent memory features.\n",
      "\n",
      "---\n",
      "\n",
      "If you have a specific subtopic or application in mind (e.g., lifelong learning, reinforcement learning agents, dialogue systems), let me know! I can drill down or recommend state-of-the-art methods, datasets, or open research questions in that area.\n"
     ]
    }
   ],
   "source": [
    "# First turn\n",
    "result_from_research_agent = await Runner.run(\n",
    "    starting_agent=research_agent,\n",
    "    input=\"Hi my name is Richmond, and I am a AI Memory Engineer researching LLMs and Agent Memory\",\n",
    "    session=session\n",
    ")\n",
    "\n",
    "print(f\"Assistant: {result_from_research_agent.final_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f1ea86e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The seminal paper that introduced the **attention mechanism** in the context of large language models (LLMs) is:\n",
      "\n",
      "---\n",
      "\n",
      "## [Attention Is All You Need](https://arxiv.org/abs/1706.03762)  \n",
      "**Authors:** Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, Illia Polosukhin  \n",
      "**Published:** 2017, NeurIPS\n",
      "\n",
      "---\n",
      "\n",
      "### **Key Contributions:**\n",
      "\n",
      "- Introduced the **Transformer** architecture, built entirely around the self-attention mechanism.\n",
      "- Showed that attention allows for efficient modeling of long-range dependencies in sequences.\n",
      "- Eliminated the need for recurrent (RNN) and convolutional layers in sequence transduction tasks.\n",
      "\n",
      "### **Influence:**\n",
      "- This paper is foundational for nearly all subsequent advancements in large language models‚Äîincluding GPT, BERT, T5, Llama, PaLM, and more.\n",
      "\n",
      "---\n",
      "\n",
      "**Citation (BibTeX):**\n",
      "```latex\n",
      "@inproceedings{vaswani2017attention,\n",
      "  title={Attention Is All You Need},\n",
      "  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, ≈Åukasz and Polosukhin, Illia},\n",
      "  booktitle={Advances in neural information processing systems},\n",
      "  pages={5998--6008},\n",
      "  year={2017}\n",
      "}\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "**If you‚Äôd like to dive deeper into self-attention, its mathematical formulation, or how it enabled the evolution of LLMs, just let me know!**\n"
     ]
    }
   ],
   "source": [
    "# Second turn\n",
    "result_from_research_agent = await Runner.run(\n",
    "    starting_agent=research_agent,\n",
    "    input=\"What is a paper that introduces the attention mechanism in LLMs?\",\n",
    "    session=session\n",
    ")\n",
    "\n",
    "print(f\"Assistant: {result_from_research_agent.final_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "144c2920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The authors of **\"Attention Is All You Need\"** are:\n",
      "\n",
      "1. **Ashish Vaswani**\n",
      "2. **Noam Shazeer**\n",
      "3. **Niki Parmar**\n",
      "4. **Jakob Uszkoreit**\n",
      "5. **Llion Jones**\n",
      "6. **Aidan N. Gomez**\n",
      "7. **≈Åukasz Kaiser**\n",
      "8. **Illia Polosukhin**\n",
      "\n",
      "These researchers were mainly at Google Brain and Google Research at the time of publication. This paper introduced the **Transformer** architecture and the self-attention mechanism, foundational for modern large language models.\n"
     ]
    }
   ],
   "source": [
    "# Third turn, the agent will remember the previous conversation\n",
    "result_from_research_agent = await Runner.run(\n",
    "    starting_agent=research_agent,\n",
    "    input=\"Who were the authors of the paper?\",\n",
    "    session=session\n",
    ")\n",
    "\n",
    "print(f\"Assistant: {result_from_research_agent.final_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "22f955bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The paper **\"Attention Is All You Need\"** was published in **2017**.\n"
     ]
    }
   ],
   "source": [
    "# Fourth turn - continuing the conversation\n",
    "result_from_research_agent = await Runner.run(\n",
    "    starting_agent=research_agent,\n",
    "    input=\"What was the year of publication?\",\n",
    "    session=session\n",
    ")\n",
    "\n",
    "print(f\"Assistant: {result_from_research_agent.final_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e1d64efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'msg_01420cb4518c1ac100690e59ffddf08190a2d779893e96bd52',\n",
       "  'content': [{'annotations': [],\n",
       "    'text': 'The paper **\"Attention Is All You Need\"** was published in **2017**.',\n",
       "    'type': 'output_text',\n",
       "    'logprobs': []}],\n",
       "  'role': 'assistant',\n",
       "  'status': 'completed',\n",
       "  'type': 'message'},\n",
       " {'content': 'What was the year of publication?', 'role': 'user'},\n",
       " {'id': 'msg_01420cb4518c1ac100690e59eb6b18819098d4279624bbc035',\n",
       "  'content': [{'annotations': [],\n",
       "    'text': 'The authors of **\"Attention Is All You Need\"** are:\\n\\n1. **Ashish Vaswani**\\n2. **Noam Shazeer**\\n3. **Niki Parmar**\\n4. **Jakob Uszkoreit**\\n5. **Llion Jones**\\n6. **Aidan N. Gomez**\\n7. **≈Åukasz Kaiser**\\n8. **Illia Polosukhin**\\n\\nThese researchers were mainly at Google Brain and Google Research at the time of publication. This paper introduced the **Transformer** architecture and the self-attention mechanism, foundational for modern large language models.',\n",
       "    'type': 'output_text',\n",
       "    'logprobs': []}],\n",
       "  'role': 'assistant',\n",
       "  'status': 'completed',\n",
       "  'type': 'message'},\n",
       " {'content': 'Who were the authors of the paper?', 'role': 'user'},\n",
       " {'id': 'msg_01420cb4518c1ac100690e59c65bbc81909d1638858358e83b',\n",
       "  'content': [{'annotations': [],\n",
       "    'text': 'The seminal paper that introduced the **attention mechanism** in the context of large language models (LLMs) is:\\n\\n---\\n\\n## [Attention Is All You Need](https://arxiv.org/abs/1706.03762)  \\n**Authors:** Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, Illia Polosukhin  \\n**Published:** 2017, NeurIPS\\n\\n---\\n\\n### **Key Contributions:**\\n\\n- Introduced the **Transformer** architecture, built entirely around the self-attention mechanism.\\n- Showed that attention allows for efficient modeling of long-range dependencies in sequences.\\n- Eliminated the need for recurrent (RNN) and convolutional layers in sequence transduction tasks.\\n\\n### **Influence:**\\n- This paper is foundational for nearly all subsequent advancements in large language models‚Äîincluding GPT, BERT, T5, Llama, PaLM, and more.\\n\\n---\\n\\n**Citation (BibTeX):**\\n```latex\\n@inproceedings{vaswani2017attention,\\n  title={Attention Is All You Need},\\n  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, ≈Åukasz and Polosukhin, Illia},\\n  booktitle={Advances in neural information processing systems},\\n  pages={5998--6008},\\n  year={2017}\\n}\\n```\\n\\n---\\n\\n**If you‚Äôd like to dive deeper into self-attention, its mathematical formulation, or how it enabled the evolution of LLMs, just let me know!**',\n",
       "    'type': 'output_text',\n",
       "    'logprobs': []}],\n",
       "  'role': 'assistant',\n",
       "  'status': 'completed',\n",
       "  'type': 'message'},\n",
       " {'content': 'What is a paper that introduces the attention mechanism in LLMs?',\n",
       "  'role': 'user'},\n",
       " {'id': 'msg_01420cb4518c1ac100690e598ca0fc8190a9c332c2c35fab9a',\n",
       "  'content': [{'annotations': [],\n",
       "    'text': 'Hello Richmond! It‚Äôs great to meet someone working at the intersection of LLMs and agent memory‚Äîa rapidly evolving and impactful field.\\n\\nHere‚Äôs a concise exploration of **LLM memory architectures** and **agent memory systems**, with relevant concepts and recent research directions:\\n\\n---\\n\\n## **Large Language Models (LLMs) and Memory**\\n\\n**1. Intrinsic Memory Limitations:**\\n- Standard large language models (e.g., GPT-4, PaLM, Llama) have a **context window**‚Äîthey can only \"remember\" a finite number of recent tokens (prompt length).\\n- Information outside this window is inaccessible unless re-provided, limiting long-term coherence and continuity.\\n\\n\\n**2. Memory Augmentation Approaches:**\\n\\n**a. External Memory Augmentation:**\\n   - **Retrieval-Augmented Generation (RAG):** LLMs query external databases, vector stores, or document indices at runtime (e.g., KNN search on embeddings), allowing access to knowledge beyond the prompt window ([Lewis et al., 2020](https://arxiv.org/abs/2005.11401)).\\n   - **Neural Turing Machines & Memory Networks:** Models with differentiable memory modules that can be read/written to support complex reasoning tasks.\\n\\n**b. Episodic Memory Architectures:**\\n   - **Long-term Episodic Storage:** Logs past interactions or events (\"episodes\") and retrieves them contextually (e.g., via semantic search) to feed into the LLM for continuity ([Park et al., 2023 ‚Äì Generative Agents](https://arxiv.org/abs/2304.03442)).\\n   - **Summarization Compression:** Long histories are summarized to create compact \"memories\" that are re-injected into the prompt (used in chatbots and open-world agents).\\n\\n**c. Recursive or Hierarchical Memory:**\\n   - Organizes memory as nested summaries, e.g., day-wise or topic-wise aggregation, to facilitate scalable long-term reasoning.\\n\\n\\n**3. Architectural Innovations:**\\n- **Transformers with Extended Context Windows:** Models like MPT-8K, Claude, Gemini extend context windows to tens-of-thousands of tokens.\\n- **Sparse Attention/Memory Layers:** Allow attending over much longer histories selectively (e.g., Longformer, Memory Transformer).\\n\\n---\\n\\n## **Agent Memory (Long-term Autonomous Agents)**\\n\\n- **Working Memory:** For immediate, short-term tasks (current plans, recent dialogue).\\n- **Episodic Memory:** Stores experiential data (events, interactions) that can be retrieved later contextually.\\n- **Semantic Memory:** Stores general world knowledge or facts (distinct from episodic).\\n- **Procedural Memory:** Encodes skills, routines, or policy behaviors.\\n\\n**Key Challenges:**\\n- **Relevance Filtering:** Selecting which memories to surface to the LLM at each step.\\n- **Catastrophic Forgetting:** Avoiding loss of old memories as new ones are added.\\n- **Efficient Indexing & Retrieval:** Fast, semantically meaningful lookup over large-scale memory banks.\\n\\n**Recent Developments:**\\n- **Generative Agents (Stanford, 2023):** Agents with memory streams that simulate personalities, routines, and relationships.\\n- **Forgetful LLMs/Selective Memory:** Techniques for intentional \"forgetting\" to manage agent memory size and relevance.\\n\\n---\\n\\n## **Resources and Further Reading**\\n\\n- **Papers:**\\n  - [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (Lewis 2020)](https://arxiv.org/abs/2005.11401)\\n  - [Generative Agents: Interactive Simulacra of Human Behavior (Park et al., 2023)](https://arxiv.org/abs/2304.03442)\\n  - [Transformers with Memory](https://arxiv.org/abs/2006.11527) (Dai et al.)\\n\\n- **Frameworks:**\\n  - [LangChain](https://langchain.com): Tools for constructing LLM-driven agents with external memory integrations.\\n  - [LlamaIndex](https://www.llamaindex.ai/): Data framework for LLM memory and search.\\n\\n- **Notable Projects:**\\n  - **AutoGPT, BabyAGI:** Open-source agents with persistent memory features.\\n\\n---\\n\\nIf you have a specific subtopic or application in mind (e.g., lifelong learning, reinforcement learning agents, dialogue systems), let me know! I can drill down or recommend state-of-the-art methods, datasets, or open research questions in that area.',\n",
       "    'type': 'output_text',\n",
       "    'logprobs': []}],\n",
       "  'role': 'assistant',\n",
       "  'status': 'completed',\n",
       "  'type': 'message'}]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the conversation subject and ensure the agent does't remember the previous conversation\n",
    "# Specifiying pop without limit will remove the last item in the session\n",
    "await session.pop_item(limit=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3a7d9dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Hello Richmond! Based on our conversation **so far**, you have not yet mentioned or referenced any specific paper. If you let me know the name or topic of the paper (or share a title, link, or abstract), I can help you by providing detailed information, a summary, or insights related to it. Please let me know more about the paper you have in mind!\n"
     ]
    }
   ],
   "source": [
    "# Fifth turn: The agent should not remember the conversations about the paper\n",
    "result_from_research_agent = await Runner.run(\n",
    "    starting_agent=research_agent,\n",
    "    input=\"What paper have we been talking about?\",\n",
    "    session=session\n",
    ")\n",
    "\n",
    "print(f\"Assistant: {result_from_research_agent.final_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c06a4705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Yes, your name is Richmond. If you have any further questions or would like to continue our discussion about LLMs and Agent Memory, feel free to let me know!\n"
     ]
    }
   ],
   "source": [
    "# Because we limited the session to a few items, the agent should still remember our name at the introduction\n",
    "result_from_research_agent = await Runner.run(\n",
    "    starting_agent=research_agent,\n",
    "    input=\"Do you still remember my name?\",\n",
    "    session=session\n",
    ")\n",
    "\n",
    "print(f\"Assistant: {result_from_research_agent.final_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3694ed8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Session conversation_123 cleared successfully.\n"
     ]
    }
   ],
   "source": [
    "# Clear the session\n",
    "await session.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cd2169c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: I don‚Äôt know your name unless you tell me in this chat. For privacy reasons, I don‚Äôt have memory of previous conversations or personal data unless you share it in our current session. If you‚Äôd like me to use your name, just let me know!\n"
     ]
    }
   ],
   "source": [
    "# Because we limited the session to 3 items, the agent should still remember our name at the introduction\n",
    "result_from_research_agent = await Runner.run(\n",
    "    starting_agent=research_agent,\n",
    "    input=\"Do you still remember my name?\",\n",
    "    session=session\n",
    ")\n",
    "\n",
    "print(f\"Assistant: {result_from_research_agent.final_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oracle_demos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
